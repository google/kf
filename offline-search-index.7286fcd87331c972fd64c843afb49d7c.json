[{"body":"","categories":"","description":"Learn how to build and deploy applications in Kf.","excerpt":"Learn how to build and deploy applications in Kf.","ref":"/kf/docs/v2.11/developer/build-and-deploy/","tags":"","title":"Build and deploy applications"},{"body":"Backing services are any processes that the App contacts over the network during its operation. In traditional operating systems, these services could have been accessed over the network, a UNIX socket, or could even be a sub-process. Examples include the following:\nDatabases — for example: MySQL, PostgreSQL File storage — for example: NFS, FTP Logging services — for example: syslog endpoints Traditional HTTP APIs — for example: Google Maps, WikiData, Parcel Tracking APIs Connecting to backing services over the network rather than installing them all into the same machine allows developers to focus on their App, independent security upgrades for different components, and flexibility to swap implementations.\nBacking services in Kf Kf supports two major types of backing services:\nManaged services: These services are created by a service broker and are tied to the Kf cluster.\nUser-provided services: These services are created outside of Kf, but get can be bound to apps in the same way as managed services.\n","categories":"","description":"Learn about how Kf works with backing services.","excerpt":"Learn about how Kf works with backing services.","ref":"/kf/docs/v2.11/developer/backing-services/overview/","tags":"","title":"Backing Services Overview"},{"body":"Kf leverages the Kubernetes Horizontal Pod Autoscaler (HPA) to automatically scale the number of Pods in a App. When autoscaling is enabled for an App, an HPA object is created and bound to the App object. It then dynamically calculates the target scale and sets it for the App.\nHow Kf scaling works The number of Pods that are deployed for a Kf App is controlled by its underlying Deployment object’s replicas field. The target number of Deployment replicas is set through the App’s replicas field.\nScaling can be done manually with the kf scale command. This command is disabled when autoscaling is enabled to avoid conflicting targets.\nHow the Kubernetes Horizontal Pod Autoscaler works The Horizontal Pod Autoscaler (HPA) is implemented as a Kubernetes API resource (the HPA object) and a control loop (the HPA controller) which periodically calculates the number of desired replicas based on current resource utilization. The HPA controller then passes the number to the target object that implements the Scale subresource. The actual scaling is delegated to the underlying object and its controller. You can find more information in the Kubernetes documentation.\nHow the Autoscaler determines when to scale Periodically, the HPA controller queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. The controller obtains the metrics from the resource metrics API for each Pod. Then the controller calculates the utilization value as a percentage of the equivalent resource request. The desired number of replicas is then calculated based on the ratio of current percentage and desired percentage. You can read more about the autoscaling algorithm in the Kubernetes documentation.\nMetrics Kf uses HPA v1 which only supports CPU as the target metric.\nHow the Kubernetes Horizontal Autoscaler works with Kf When autoscaling is enabled for a Kf App, the Kf controller will create an HPA object based on the scaling limits and rules specified on the App. Then the HPA controller fetches the specs from the HPA object and scales the App accordingly.\nThe HPA object will be deleted if Autoscaling is disabled or if the corresponding App is deleted.\n","categories":"","description":"Learn about how Kf scales apps.","excerpt":"Learn about how Kf scales apps.","ref":"/kf/docs/v2.11/developer/scaling/autoscaling/","tags":"","title":"Scaling overview"},{"body":"About Tasks Unlike Apps which run indefinitely and restart if the process terminates, Tasks run a process until it completes and then stop. Tasks are run in their own containers and are based on the configuration and binary of an existing App.\nTasks are not accessible from routes, and should be used for one-off or scheduled recurring work necessary for the health of an application.\nUse cases for Tasks Migrating a database Running a batch job (scheduled/unscheduled) Sending an email Transforming data (ETL) Processing data (upload/backup/download) How Tasks work Tasks are executed asynchronously and run independently from the parent App or other Tasks running on the same App. An App created for running Tasks does not have routes created or assigned, and the Run lifecycle is skipped. The Source code upload and Build lifecycles still proceed and result in a container image used for running Tasks after pushing the App (see App lifecycles at Deploying an Application).\nThe lifecycle of a Task is as follows:\nYou push an App for running tasks with the kf push APP_NAME --task command. You run a Task on the App with the kf run-task APP_NAME command. Task inherits the environment variables, service bindings, resource allocation, start-up command, and security groups bound to the App. Kf creates a Tekton PipelineRun with values from the App and parameters from the run-task command. The Tekton PipelineRun creates a Kubernetes Pod which launches a container based on the configurations on the App and Task. Task execution stops (Task exits or is terminated manually), the underlying Pod is either stopped or terminated. Pods of stopped Tasks are preserved and thus Task logs are accessible via the kf logs APP_NAME --task command. If you terminate a Task before it stops, the Tekton PipelineRun is cancelled (see Cancelling a PipelineRun), the underlying Pod together with the logs are deleted. The logs of termianted Tasks are delivered to the cluster level logging streams if configured (e.g. Stackdriver, Fluentd). If the number of Tasks run on an App is greater than 500, the oldest Tasks are automatically deleted. Tasks retention policy Tasks are created as custom resources in the Kubernetes cluster, therefore, it is important not to exhaust the space of the underlying etcd database. By default, Kf only keeps the latest 500 Tasks per each App. Once the number of Tasks reach 500, the oldest Tasks (together with the underlying Pods and logs) will be automatically deleted.\nTask logging and execution history Any data or messages the Task outputs to STDOUT or STDERR is available by using the kf logs APP_NAME --task command. Cluster level logging mechanism (such as Stackdriver, Fluentd) will deliver the Task logs to the configured logging destination.\nScheduling Tasks As described above, Tasks can be run asynchronously by using the kf run-task APP_NAME command. Alternatively, you can schedule Tasks for execution by first creating a Job using the kf create-job command, and then scheduling it with the kf schedule-job JOB_NAME command. You can schedule that Job to automatically run Tasks on a specified unix-cron schedule.\nHow Tasks are scheduled Create and schedule a Job to run the Task. A Job describes the Task to execute and automatically manages Task creation.\nTasks are created on the schedule even if previous executions of the Task are still running. If any executions are missed for any reason, only the most recently missed execution are executed when the system recovers.\nDeleting a Job deletes all associated Tasks. If any associated Tasks were still in progress, they are forcefully deleted without running to completion.\nTasks created by a scheduled Job are still subject to the Task retention policy.\nDifferences from PCF Scheduler PCF Scheduler allows multiple schedules for a single Job while Kf only supports a single schedule per Job. You can replicate the PCF Scheduler behavior by creating multiple Jobs, one for each schedule.\n","categories":"","description":"Understand how tasks work in Kf.\n","excerpt":"Understand how tasks work in Kf.\n","ref":"/kf/docs/v2.11/developer/tasks/task/","tags":"","title":"Tasks Overview"},{"body":"","categories":"","description":"Get started with Kf as an app runtime.\n","excerpt":"Get started with Kf as an app runtime.\n","ref":"/kf/docs/v2.11/getting-started/","tags":"","title":"Getting Started"},{"body":"With Kf, you can migrate your Cloud Foundry apps to Kubernetes without changing your developer workflows.\nUse the following sections to get started deploying Cloud Foundry apps to Kf.\nGet started with a quickstart Use the quickstart to install Kf and deploy a simple test Cloud Foundry app. This introduces you to the basic steps you’d perform for most app deployments.\nGet an introduction to key concepts For an introduction to the value of Kf, as well as high-level overviews of the technology, see the following documents:\nTo learn more about how Kf works, see Kf dependencies and architecture. For a side-by-side comparison of Cloud Foundry and Kf components, see Compare Cloud Foundry and Kf services. To learn how to use service brokers with Kf, see Service brokers. For more information about Kf security design, see the Security overview. ","categories":"","description":"","excerpt":"With Kf, you can migrate your Cloud Foundry apps to Kubernetes without …","ref":"/kf/docs/v2.11/getting-started/overview/","tags":"","title":"Getting Started Overview"},{"body":"","categories":"","description":"Learn to install and operate backing service marketplaces for Kf.","excerpt":"Learn to install and operate backing service marketplaces for Kf.","ref":"/kf/docs/v2.11/operator/service-brokers/","tags":"","title":"Service brokers"},{"body":"Kf Apps can be automatically scaled based on CPU usage. You can configure autoscaling limits for your Apps and the target CPU usage for each App instance. Kf automatically scales your Apps up and down in response to demand.\nBy default, autoscaling is disabled. Follow the steps below to enable autoscaling.\nView Apps You can view the autoscaling status for an App using the kf apps command. If autoscaling is enabled for an App, Instances includes the autoscaling status.\n$ kf apps Name Instances Memory Disk CPU app1 4 (autoscaled 4 to 5) 256Mi 1Gi 100m app2 1 256Mi 1Gi 100m Autoscaling is enabled for app1 with min-instances set to 4 and max-instances set to 5. Autoscaling is disabled for app2.\nUpdate autoscaling limits You can update the instance limits using the kf update-autoscaling-limits command.\nkf update-autoscaling-limits app-name min-instances max-instances Create autoscaling rule You can create autoscaling rules using the kf create-autoscaling-rule command.\nkf create-autoscaling-rule app-name CPU min-threshold max-threshold Delete autoscaling rules You can delete all autoscaling rules with the kf delete-autoscaling-rule command. Kf only supports one autoscaling rule.\nkf delete-autoscaling-rules app-name Enable and disable autoscaling Autoscaling can be enabled by using enable-autoscaling and disabled by using disable-autoscaling. When it is disabled, the configurations, including limits and rules, are preserved.\nkf enable-autoscaling app-name kf disable-autoscaling app-name ","categories":"","description":"Learn to use autoscaling for your app.","excerpt":"Learn to use autoscaling for your app.","ref":"/kf/docs/v2.11/developer/scaling/manage-autoscaling/","tags":"","title":"Manage Autoscaling"},{"body":"In this quickstart, you will deploy a sample Cloud Foundry app on an existing Kf cluster.\nPush an application Prerequisites The following are required to complete this section:\nThe kf CLI installed and in your path.\nYou have connected to the Kf Kubernetes cluster:\ngcloud container clusters get-credentials CLUSTER_NAME \\ --project=CLUSTER_PROJECT_ID \\ --zone=CLUSTER_LOCATION The git CLI installed and in your path.\nPrepare space Create new space:\nkf create-space test-space Target the space:\nkf target -s test-space Push the Cloud Foundry test app Clone the test-app repo.\ngit clone https://github.com/cloudfoundry-samples/test-app go-test-app cd go-test-app Push the app.\nNote: It will take a few minutes for the build to complete. kf push test-app Get the application’s URL.\nkf apps Note: The app will have a random route by default. Open the URL in your browser where you should see the app running.\nSuccessful app push on Kf Clean up These steps should return the cluster to the starting state.\nDelete the application.\nkf delete test-app Delete the Space.\nkf delete-space test-space ","categories":"","description":"","excerpt":"In this quickstart, you will deploy a sample Cloud Foundry app on an …","ref":"/kf/docs/v2.11/getting-started/quickstart/","tags":"","title":"Quickstart"},{"body":"This document provides a side-by-side comparison of the various services available on Cloud Foundry (CF) and those that Kf integrates with on Google Cloud.\nNote: This guide does not attempt to compare the syntax or semantics of the SDK, APIs, or command-line tools provided by CF and Kf. Service category Service CF Kf Platform Infrastructure Orchestrator BOSH Kubernetes PaaS CF Application Runtime (CFAR) Kf Data management Service Broker Service Broker Tile Kubernetes Deployed Service Brokers MySQL MySQL Tile Kf Cloud Service Broker MongoDB MongoDB Tile Kf Cloud Service Broker RabbitMQ RabbitMQ Tile Kf Cloud Service Broker Redis Redis Tile Kf Cloud Service Broker Eureka Spring Cloud Services Tile Service Discovery Spring Cloud Config Spring Cloud Services Tile Spring Cloud Config Operations tooling Continuous Integration (CI) Concourse Tile Concourse Helm Chart Logging Google Cloud Google Cloud Firehose Nozzle Google Cloud Logging Kubernetes Agent Elastic Elastic Firehose Nozzle Elastic Stack Agent Splunk Splunk Firehose Nozzle Splunk Connect Metrics CF App Metrics Google Cloud Monitoring Kubernetes AGent ","categories":"","description":"","excerpt":"This document provides a side-by-side comparison of the various …","ref":"/kf/docs/v2.11/getting-started/compare-services/","tags":"","title":"Compare Cloud Foundry and Kf services"},{"body":"","categories":"","description":"Discover, create, and use backing services.\n","excerpt":"Discover, create, and use backing services.\n","ref":"/kf/docs/v2.11/developer/backing-services/","tags":"","title":"Backing services"},{"body":"Find a service Use the kf marketplace command to find a service you want to use in your App. Running the command without arguments will show all the service classes available. A service class represents a specific type of service e.g. a MySQL database or a Postfix SMTP relay.\n$ kf marketplace 5 services can be used in Space \"test\", use the --service flag to list the plans for a service Broker Name Space Status Description minibroker mariadb Active Helm Chart for mariadb minibroker mongodb Active Helm Chart for mongodb minibroker mysql Active Helm Chart for mysql minibroker postgresql Active Helm Chart for postgresql minibroker redis Active Helm Chart for redis Service classes can have multiple plans available. A service plan generally corresponds to a version or pricing tier of the software. You can view the plans for a specific service by supplying the service name with the marketplace command:\n$ kf marketplace --service mysql Name Free Status Description 5-7-14 true Active Fast, reliable, scalable, and easy to use open-source relational database system. 5-7-27 true Active Fast, reliable, scalable, and easy to use open-source relational database system. 5-7-28 true Active Fast, reliable, scalable, and easy to use open-source relational database system. Provision a service Once you have identified a service class and plan to provision, you can create an instance of the service using kf create-service:\n$ kf create-service mysql 5-7-28 my-db Creating service instance \"my-db\" in Space \"test\" Waiting for service instance to become ready... Success Services are provisioned into a single Space. You can see the services in the current Space by running kf services:\n$ kf services Listing services in Space: \"test\" Name ClassName PlanName Age Ready Reason my-db mysql 5-7-28 111s True \u003cnil\u003e You can delete a service using kf delete-service:\n$ kf delete-service my-db Bind a service Once a service has been created, you can bind it to an App, which will inject credentials into the App so the service can be used. You can create the binding using kf bind-service:\n$ kf bind-service my-app my-db Creating service instance binding \"binding-my-app-my-db\" in Space \"test\" Waiting for service instance binding to become ready... Success You can list all bindings in a Space using the kf bindings command:\n$ kf bindings Listing bindings in Space: \"test\" Name App Service Age Ready binding-my-app-my-db my-app my-db 82s True Once a service is bound, restart the App using kf restart and the credentials will be in the VCAP_SERVICES environment variable.\nYou can delete a service binding with the kf unbind-service command:\n$ kf unbind-service my-app my-db ","categories":"","description":"Learn to use the marketplace to find a service, create it, and bind it to an app.\n","excerpt":"Learn to use the marketplace to find a service, create it, and bind it …","ref":"/kf/docs/v2.11/developer/backing-services/managed-services/","tags":"","title":"Use managed services"},{"body":"","categories":"","description":"Learn to scale apps up or down.\n","excerpt":"Learn to scale apps up or down.\n","ref":"/kf/docs/v2.11/developer/scaling/","tags":"","title":"Scaling"},{"body":"Users can leverage services that aren’t available in the marketplace by creating user-provided service instances. Once created, user-provided service instances behave like managed service instances created through kf create-service. Creating, listing, updating, deleting, binding, and unbinding user-provided services are all supported in Kf.\nCreate a user-provided service instance Tip: The alias for kf create-user-provided-service is kf cups. The name given to a user-provided service must be unique across all service instances in a Space, including services created through a service broker.\nDeliver service credentials to an app A user-provided service instance can be used to deliver credentials to an App. For example, a database admin can have a set of credentials for an existing database managed outside of Kf, and these credentials include the URL, port, username, and password used to connect to the database.\nThe admin can create a user-provided service with the credentials and the developer can bind the service instance to the App. This allows the credentials to be shared without ever leaving the platform. Binding a service instance to an App has the same effect regardless of whether the service is a user-provided service or a marketplace service.\nThe App is configured with the credentials provided by the user, and the App runtime environment variable VCAP_SERVICES is populated with information about all of the bound services to that App.\nA user-provided service can be created with credentials and/or a list of tags.\nkf cups my-db -p '{\"username\":\"admin\", \"password\":\"test123\", \"some-int-val\": 1, \"some-bool\": true}' -t \"comma, separated, tags\" This will create the user-provided service instance my-db with the provided credentials and tags. The credentials passed in to the -p flag must be valid JSON (either inline or loaded from a file path).\nTo deliver the credentials to one or more Apps, the user can run kf bind-service.\nSuppose we have an App with one bound service, the user-provided service my-db defined above. The VCAP_SERVICES environment variable for that App will have the following contents:\n{ \"user-provided\": [ { \"name\": \"my-db\", \"instance_name\": \"my-db\", \"label\": \"user-provided\", \"tags\": [ \"comma\", \"separated\", \"tags\" ], \"credentials\": { \"username\": \"admin\", \"password\": \"test123\", \"some-int-val\": 1, \"some-bool\": true } } ] } Update a user-provided service instance Tip: The alias for kf update-user-provided-service is kf uups. A user-provided service can be updated with the uups command. New credentials and/or tags passed in completely overwrite existing ones. For example, if the user created the user-provided service my-db above, called kf bind-service to bind the service to an App, then ran the command.\nkf uups my-db -p '{\"username\":\"admin\", \"password\":\"new-pw\", \"new-key\": \"new-val\"}' The updated credentials will only be reflected on the App after the user unbinds and rebinds the service to the App. No restart or restage of the App is required. The updated VCAP_SERVICES environment variable will have the following contents:\n{ \"user-provided\": [ { \"name\": \"my-db\", \"instance_name\": \"my-db\", \"label\": \"user-provided\", \"tags\": [ \"comma\", \"separated\", \"tags\" ], \"credentials\": { \"username\": \"admin\", \"password\": \"new-pw\", \"new-key\": \"new-val\" } } ] } The new credentials overwrite the old credentials, and the tags are unchanged because they were not specified in the update command.\n","categories":"","description":"Learn how to inject existing services into your app.\n","excerpt":"Learn how to inject existing services into your app.\n","ref":"/kf/docs/v2.11/developer/backing-services/user-provided-services/","tags":"","title":"Configure user-provided services"},{"body":"","categories":"","description":"Learn about how to configure cluster-wide settings.","excerpt":"Learn about how to configure cluster-wide settings.","ref":"/kf/docs/v2.11/operator/customizing/","tags":"","title":"Customizing Kf"},{"body":"When pushing an app (via kf push) to Kf, there are three lifecycles that Kf uses to take your source code and allow it to handle traffic:\nSource code upload Build Run Source code upload The first thing that happens when you kf push is the Kf CLI (kf) packages up your directory (either current or --path/-p) into a container and publishes it to the container registry configured for the Space. This is called the source container. The Kf CLI then creates an App type in Kubernetes that contains both the source image and configuration from the App manifest and push flags.\nIgnore files during push In many cases, you will not want to upload certain files during kf push (i.e., “ignore” them). This is where a .kfignore (or .cfignore) file can be used. Similar to a .gitignore file, this file instructs the Kf CLI which files to not include in the source code container.\nTo create a .kfignore file, create a text file named .kfignore in the base directory of your app (similar to where you would store the manifest file). Then populate it with a newline delimited list of files and directories you don’t want published. For example:\nbin .idea This will tell the Kf CLI to not include anything in the bin or .idea directories.\nKf supports gitignore style syntax.\nBuild The Build lifecycle is handled by a Tekton TaskRun. Depending on the flags that you provide while pushing, it will choose a specific Tekton Task. Kf currently has the following Tekton Tasks:\nbuildpackv2 buildpackv3 kaniko Kf tracks each TaskRun as a Build. If a Build succeeds, the resulting container image is then deployed via the Run lifecycle (described below).\nMore information can be found at Build runtime.\nRun The Run lifecycle is responsible for taking a container image and creating a Kubernetes Deployment.\nIt also creates:\nIstio Virtual Services Kubernetes Secrets More information can be found at Build runtime.\nPush timeouts Kf supports setting an environment variable to instruct the CLI to time out while pushing apps. If set, the variables KF_STARTUP_TIMEOUT or CF_STARTUP_TIMEOUT are parsed as a golang style duration (for example 15m, 1h). If a value is not set, the push timeout defaults to 15 minutes.\n","categories":"","description":"Guide to deploying applications with Kf.","excerpt":"Guide to deploying applications with Kf.","ref":"/kf/docs/v2.11/developer/build-and-deploy/deploying-an-app/","tags":"","title":"Deploy an application"},{"body":"","categories":"","description":"Learn to use Kf to deploy and maintain applications.","excerpt":"Learn to use Kf to deploy and maintain applications.","ref":"/kf/docs/v2.11/developer/","tags":"","title":"Develop Applications on Kf"},{"body":"Kf supports customizing some cluster wide configurations by manipulating the kfsystem custom rsource.\nWarning: Changes will be instantly pushed to all Kf resources, test any customization options before using them in production. ","categories":"","description":"Learn about how Kf uses configuration.","excerpt":"Learn about how Kf uses configuration.","ref":"/kf/docs/v2.11/operator/customizing/about-customizing-kf/","tags":"","title":"Customizing Overview"},{"body":"Kf aims to provide a similar developer experience to Cloud Foundry, replicating the build, push, and deploy lifecycle. It does this by building a developer UX layer on top of widely-known, broadly used and adopted technologies like Kubernetes, Istio, and container registries rather than by implementing all the pieces from the ground up.\nNote: Kf should be used in a GCP Project dedicated to your evaluation. See Important considerations for more information. When making security decisions, Kf aims to provide complete solutions that are native to their respective components and can be augmented with other mechanisms. Breaking that down:\nComplete solutions means that Kf tries not to provide partial solutions that can lead to a false sense of security. Native means that the solutions should be a part of the component rather than a Kf construct to prevent breaking changes. Can be augmented means the approach Kf takes should work seamlessly with other Kubernetes and Google Cloud tooling for defense in depth. Important considerations In addition to the Current limitations described below, it is important that you read through and understand the items outlined in this section.\nWorkload Identity By default, Kf uses Workload Identity to provide secure delivery and rotation of the Service Account credentials used by Kf to interact with your Google Cloud Project. Workload Identity achieves this by mapping a Kubernetes Service Account (KSA) to a Google Service Account (GSA). The Kf controller runs in the kf namespace and uses a KSA named controller mapped to your GSA to do the following things:\nWrite metrics to Stackdriver When a new Kf space is created (kf create-space), the Kf controller creates a new KSA named kf-builder in the new space and maps it to the same GSA. The kf-builder KSA is used by Tekton to push and pull container images to Google Container Registry (gcr.io) This diagram illustrates those interactions:\nCurrent limitations Kf doesn’t provide pre-built RBAC roles. Until Kf provides this, use RBAC.\nA developer pushing an app with Kf can also create Pods (with kubectl) that can use the kf-builder KSA with the permissions of its associated GSA.\nDeploying to Kf requires write access to a container registry. Deploy Kf in a dedicated project without access to production resources. Grant developers access to push code to the Artifact Repository by granting them roles/storage.admin on the project or buckets that Artifact Repository uses.\nKf uses the same Pod to fetch, build, and store images. Assume that any credentials that you provide can be known by the authors and publishers of the buildpacks you use.\nKf doesn’t support quotas to protect against noisy neighbors. Use Kubernetes resource quotas.\nOther resources Google Cloud General GKE security overview GKE cluster multi-tenancy Workload Identity GKE and Cloud IAM policies Recommended protections Protecting cluster metadata Role-based access control Advanced protections GKE Sandbox Network policies ","categories":"","description":"Unerstand Kf's security posture.","excerpt":"Unerstand Kf's security posture.","ref":"/kf/docs/v2.11/operator/security/security-overview/","tags":"","title":"Security Overview"},{"body":"Kf supports binding and provisioning apps to Open Service Broker (OSB) services.\nAny compatible service broker can be installed using the create-service-broker command, but only the Kf Cloud Service Broker is fully supported.\nNote: Kf only supports a subset of the types of services that Open Service Brokers provide. Specifically, it only supports credential services. Special services such as syslog drains, volume services, route services, service keys, and shared services aren’t currently supported.\n","categories":"","description":"","excerpt":"Kf supports binding and provisioning apps to Open Service Broker (OSB) …","ref":"/kf/docs/v2.11/operator/service-brokers/about-service-brokers/","tags":"","title":"Service Brokers Overview"},{"body":"You can execute short-lived workflows by running them as Tasks in Kf. Tasks are run under Apps, meaning that each Task must have an associated App. Each Task execution uses the build artifacts from the parent App. Because Tasks are short-lived, the App is not deployed as a long-running application, and no routes should be created for the App or the Task.\nPush an App for running Tasks Clone the test-app repo repo:\ngit clone https://github.com/cloudfoundry-samples/test-app test-app cd test-app Push the App.\nPush the App with the kf push APP_NAME --task command. The --task flag indicates that the App is meant to be used for running Tasks, and thus no routes are created on the App, and it is not deployed as a long-running application:\nkf push test-app --task Confirm that no App instances or routes were created by listing the App:\nkf apps Notice that the App is not started and has no URLs:\nListing Apps in Space: test-space Name Instances Memory Disk CPU URLs test-app stopped 1Gi 1Gi 100m \u003cnil\u003e Run a Task on the App When you run a Task on the App, you can optionally specify a start command by using the --command flag. If no start command is specified, it uses the start command specified on the App. If the App doesn’t have a start command specified, it looks up the CMD configuration of the container image. A start command must exist in order to run the Task successfully.\nkf run-task test-app --command \"printenv\" You see something similar to this, confirming that the Task was submitted:\nTask test-app-gd8dv is submitted successfully for execution. The Task name is automatically generated, prefixed with the App name, and suffixed with an arbitrary string. The Task name is a unique identifier for Tasks within the same cluster.\nSpecify Task resource limits Resource limits (such as CPU cores/Memory limit/Disk quota) can be specified in the App (during kf push) or during the kf run-task command. The limits specified in the kf run-task command take prededence over the limits specified on the App.\nTo specify resource limits in an App, you can use the --cpu-cores, --memory-limit, and --disk-quota flags in the kf push command:\nkf push test-app --command \"printenv\" --cpu-cores=0.5 --memory-limit=2G --disk-quota=5G --task To override these limits in the App, you can use the --cpu-cores, --memory-limit, and --disk-quota flags in the kf run-task command:\nkf run-task test-app --command \"printenv\" --cpu-cores=0.5 --memory-limit=2G --disk-quota=5G Specify a custom display name for a Task You can optionally use the --name flag to specify a custom display name for a Task for easier identification/grouping:\n$ kf run-task test-app --command \"printenv\" --name foo Task test-app-6swct is submitted successfully for execution. $ kf tasks test-app Listing Tasks in Space: test space Name ID DisplayName Age Duration Succeeded Reason test-app-6swct 3 foo 1m 21s True \u003cnil\u003e Manage Tasks View all Tasks of an App with the kf tasks APP_NAME command:\n$ kf tasks test-app Listing Tasks in Space: test space Name ID DisplayName Age Duration Succeeded Reason test-app-gd8dv 1 test-app-gd8dv 1m 21s True \u003cnil\u003e Cancel a Task Cancel an active Task by using the kf terminate-task command:\nCancel a Task by Task name:\n$ kf terminate-task test-app-6w6mz Task \"test-app-6w6mz\" is successfully submitted for termination Or cancel a Task by APP_NAME + Task ID:\n$ kf terminate-task test-app 2 Task \"test-app-6w6mz\" is successfully submitted for termination Note: You can only cancel Tasks that are pending/running. Completed Tasks are not cancellable. Cancelled Tasks have PipelineRunCancelled status.\n$ kf tasks test-app Listing Tasks in Space: test space Name ID DisplayName Age Duration Succeeded Reason test-app-gd8dv 1 test-app-gd8dv 1m 21s True \u003cnil\u003e test-app-6w6mz 2 test-app-6w6mz 38s 11s False PipelineRunCancelled View Task logs View logs of a Task by using the kf logs APP_NAME --task command:\n$ kf logs test-app --task ","categories":"","description":"Learn how to use tasks to run one-off jobs.\n","excerpt":"Learn how to use tasks to run one-off jobs.\n","ref":"/kf/docs/v2.11/developer/tasks/run-task/","tags":"","title":"Run Tasks"},{"body":"This document is an overview of Kubernetes DNS-based service discovery and how it can be used with Kf.\nWhen to use Kubernetes service discovery with Kf Kubernetes service discovery can be used by applications that need to locate backing services in a consistent way regardless of where the application is deployed. For example, a team might want to use a common URI in their configuration that always points at the local SMTP gateway to decouple code from the environment it ran in.\nService discovery helps application teams by:\nReducing the amount of per-environment configuration. Decoupling client and server applications. Allowing applications to be portable to new environments. You can use Kubernetes service discovery when:\nApplications use their container’s DNS configurations to resolve hosts. Applications are deployed with their backing services in the same Kubernetes cluster or namespace. Backing services have an associated Kubernetes service. Kf creates these for each app. Kubernetes NetworkPolicies allow traffic between an application and the Kubernetes service it needs to communicate with. Kf creates these policies in each Kf space. You should not use Kubernetes service discovery if:\nApplications need to failover between multiple clusters. You override the DNS resolver used by your application. Applications need specific types of load balancing. How Kubernetes service discovery works Kubernetes service discovery works by modifying the DNS configuration of containers running on a Kubernetes node. When an application looks up an unqualified domain name, the local DNS resolver will first attempt to resolve the name in the local cluster.\nDomains without multiple parts will be resolved against the names of Kubernetes services in the container’s namespace. Each Kf app creates a Kubernetes service with the same name. If two Kf apps ping and pong were deployed in the same Kf space, then ping could use the URL http://pong to send traffic to the other service.\nDomains with a single dot will be resolved against the Kubernetes services in the Kubernetes namespace with the same name as the label after the dot. For example, if there was a PostgreSQL database with a customers service in the database namespace, an application in another namespace could resolve it using postgres://customers.database.\nHow to use service discovery with Kf Kubernetes DNS based service discovery can be used in any Kf app. Each Kf app creates a Kubernetes service of the same name, and each Kf space creates a Kubernetes namespace with the same name.\nRefer to a Kf app in the current space using protocol://app-name. Refer to a Kf app in a different space using protocol://app-name.space-name. Refer to a Kf app in the current space listening on a custom port using protocol://app-name:port. Refer to a Kf app in a different space listening a custom port using protocol://app-name.space-name:port. Note: For Kubernetes services managed by Kf, TCP port 80 always maps to the same port in the app’s PORT environment variable. This is even true if the Kf app doesn’t serve HTTP traffic. Best practices Applications that are going to be the target of DNS based service discovery should have frequent health checks to ensure they are rapidly added and removed from the pool of hosts that accept connections.\nApplications using DNS based service discovery should not cache the IP addresses of the resolved services because they are not guaranteed to be stable.\nIf environment specific services exist outside of the cluster, they can be resolved using Kubernetes DNS if you set up ExternalName Kubernetes services. These Kubernetes services provide the same resolution capabilities, but return a CNAME record to redirect requests to an external authority.\nComparison to Eureka Eureka is an open source client-side load-balancer created by Netflix. It is commonly used as part of the Spring Cloud Services service broker. Eureka was built to be a regional load balancer and service discovery mechanism for services running in an environment that caused frequent disruptions to workloads leading to unstable IP addresses.\nEureka is designed as a client/server model. Clients register themselves with the server indicating which names they want to be associated with and periodically send the server heartbeats. The server allows all connected clients to resolve names.\nIn general, you should use Kubernetes DNS rather than Eureka in Kubernetes for the following reasons:\nDNS works with all programming languages and applications without the need for libraries. Your application’s existing health check will be reused reducing combinations of errors. Kubernetes manages the DNS server, allowing you to rely on fewer dependencies. Kubernetes DNS respects the same policy and RBAC constraints as the rest of Kubernetes. There are a few times when deploying a Eureka server would be advantageous:\nYou need service discovery across Kubernetes and VM based applications. You need client based load-balancing. You need independent health checks. What’s next Read more about service discovery in GKE. Learn about Service Directory, a managed offering similar to Eureka. ","categories":"","description":"Connect to other Apps in the Kf cluster.","excerpt":"Connect to other Apps in the Kf cluster.","ref":"/kf/docs/v2.11/developer/service-discovery/service-discovery/","tags":"","title":"Service discovery"},{"body":" Note: You can leverage services that aren’t listed in the marketplace by creating user-provided service instances that an App can bind to. Learn more about user-provided services. Before you begin Ensure your service is running and accessible on the same network running your Kf cluster. Ensure you have targeted the Space where you want to create the service. Create the user-provided instance The following examples use the most common parameters used by applications to autoconfigure services. Most libraries use tags to find the right bound service and a URI to connect.\nMySQL MySQL libraries usually expect the tag mysql and the following parameters:\nuri Example mysql://username:password@host:port/dbname. The MySQL documentation can help with creating a URI string. The port is usually 3306. username The connection username, required by some libraries even if included in uri. password The connection password, required by some libraries even if included in uri. kf cups service-instance-name \\ -p '{\"username\":\"my-username\", \"password\":\"my-password\", \"uri\":\"mysql://my-username:my-password@mysql-host:3306/my-db\"}' \\ -t \"mysql\" RabbitMQ RabbitMQ libraries usually expect the tag rabbitmq and the following parameters:\nuri Example amqp://username:password@host:port/vhost?query. The RabbitMQ documentation can help with creating a URI string. The port is usually 5672. Example:\nkf cups service-instance-name \\ -p '{\"uri\":\"amqp://username:password@host:5672\"}' \\ -t \"rabbitmq\" Redis Redis libraries usually expect the tag redis and the following parameters:\nuri Example redis://:password@host:port/uery. The IANA Redis URI documentation can help with creating a URI string. The port is usually 6379. Example for Redis with no AUTH configured:\nkf cups service-instance-name \\ -p '{\"uri\":\"redis://redis-host:6379\"}' \\ -t \"redis\" Example for Redis with AUTH configured:\nkf cups service-instance-name \\ -p '{\"uri\":\"redis://:password@redis-host:6379\"}' \\ -t \"redis\" Bind your App Once the user-provided service has been created, you can bind your App to the user provided service by name, just like a managed service:\nkf bind-service application-name service-instance-name What’s next Learn about how the credentials are injected into your app. ","categories":"","description":"Learn how to set up user provided services for MySQL, Redis, and RabbitMQ.","excerpt":"Learn how to set up user provided services for MySQL, Redis, and …","ref":"/kf/docs/v2.11/developer/backing-services/ups-templates/","tags":"","title":"User Provided Service Templates"},{"body":"Kf supports mounting NFS volumes using the kf marketplace.\nPrerequisites Your administrator must have completed the NFS platform setup guide. Create an NFS service instance Run kf marketplace to see available services. The built-in NFS service appears on the list if NFS is enabled on the platform.\nBroker Name Namespace Description nfsvolumebroker nfs mount nfs shares Mount an external filesystem Create a service instance To mount to an existing NFS service:\nkf create-service nfs existing SERVICE-INSTANCE-NAME -c '{\"share\":\"SERVER/SHARE\", \"capacity\":\"CAPACITY\"}' Replace variables with your values.\nSERVICE-INSTANCE-NAME is the name you want for this NFS volume service instance. SERVER/SHARE is the NFS address of your server and share. CAPACITY uses the Kubernetes quantity format. Confirm that the NFS volume service appears in your list of services. You can expect output similar to this example:\n$ kf services ... Listing services in Space: demo-space Name Type ClassName PlanName Age Ready Reason filestore-nfs volume nfs existing 6s True \u003cnil\u003e ... Bind your service instance to an App To bind an NFS service instance to an App, run:\nkf bind-service YOUR-APP-NAME SERVICE-NAME -c '{\"uid\":\"2000\",\"gid\":\"2000\",\"mount\":\"MOUNT-PATH\",\"readonly\":true}' Replace variables with your values.\nYOUR-APP-NAME is the name of the App for which you want to use the volume service.\nSERVICE-NAME is the name of the volume service instance you created in the previous step.\nuid:UID and gid:GID specify the directory permissions of the mounting share.\nNote: For V2 buildpack Apps, the value for uid and gid is always 2000. Otherwise, the user specified by uid and gid should be the uid and gid of the running App process. MOUNT-PATH is the path the volume should be mounted to within your App.\n(Optional) \"readonly\":true is an optional JSON string that creates a read-only mount. By default, Volume Services mounts a read-write file system.\nNote: Your App automatically restarts when the NFS binding changes.\nYou can list all bindings in a Space using the kf bindings command. You will see output similar to this example:\n$ kf bindings ... Listing bindings in Space: demo-space Name App Service Age Ready binding-spring-music-filestore-nfs spring-music filestore-nfs 71s True ... Access the volume service from your App To access the volume service from your App, you must know which file path to use in your code. You can view the file path in the details of the service binding, which are visible in the environment variables for your App.\nView environment variables for your App:\nkf vcap-services YOUR-APP-NAME Replace YOUR-APP-NAME with the name of your App.\nThe following is example output of the kf vcap-services command:\n$ kf vcap-services YOUR-APP-NAME { \"nfs\": [ { \"instance_name\": \"nfs-instance\", \"name\": \"nfs-instance\", \"label\": \"nfs\", \"tags\": [], \"plan\": \"existing\", \"credentials\": { \"capacity\": \"1Gi\", \"gid\": 2000, \"mount\": \"/test/mount\", \"share\": \"10.91.208.210/test\", \"uid\": 2000 }, \"volume_mounts\": [ { \"container_dir\": \"/test/mount\", \"device_type\": \"shared\", \"mode\": \"rw\" } ] } ] } Use the properties under volume_mounts for any information required by your App.\nProperty Description container_dir String containing the path to the mounted volume that you bound to your App. device_type The NFS volume release. This currently only supports shared devices. A shared device represents a distributed file system that can mount on all App instances simultaneously. mode String that informs what type of access your App has to NFS, either ro (read-only), or rw (read and write). ","categories":"","description":"Learn how to use an NFS volume as a mounted drive.","excerpt":"Learn how to use an NFS volume as a mounted drive.","ref":"/kf/docs/v2.11/developer/backing-services/nfs-getting-started/","tags":"","title":"Configure NFS volumes"},{"body":" Warning: Changes will be instantly pushed to all Kf resources, test any customization options before using them in production. Kf uses a Kubernetes configmap named config-defaults in the kf namespace to store cluster wide configuration settings. This document explains its structure and fields.\nStructure of the config-defaults configmap The configmap contains three types of key/value pairs in the .data field:\nComment keys prefixed by _ contain examples, notes, and warnings. String keys contain plain text values. Object keys contain a JSON or YAML value that has been encoded as a string. Example:\n_note: \"This is some note\" stringKey: \"This is a string key that's not encoded as JSON or YAML.\" objectKey: | - \"These keys contain nested YAML or JSON.\" - true - 123.45 Note: Kubernetes requires all configmap values to be strings. Kf validates the structure of the fields before the object is updated. Example section The example section under the _example key contains explanations for other fields and examples. Changes to this section have no effect.\nSpace container registry The spaceContainerRegistry property is a plain text value that specifies the default container registry each space uses to store built images.\nExample:\nspaceContainerRegistry: gcr.io/my-project Space cluster domains The spaceClusterDomains property is a string encoded YAML array of domain objects.\nEach space in the cluster adds all items in the array to its list of domains that developers can bind their apps to.\nFields domain string\nThe domain name to make available. May contain one of the following substitutions:\n$(SPACE_NAME) - Replaced in each space with the name of the space. $(CLUSTER_INGRESS_IP) - The IP address of the cluster ingress gateway. gatewayName string\n(Optional)\nOverrides the Istio gateway routes will be bound to. Defaults to kf/external-gateway, but any other gateway in the kf namespace may be used. Note: All domains should contain the $(SPACE_NAME) substitution to prevent Apps in different namespaces from listening to the same hostname which causes undefined behavior. Example:\nspaceClusterDomains: | # Support canonical and vanity domains - domain: $(SPACE_NAME).prod.example.com - domain: $(SPACE_NAME).kf.us-east1.prod.example.com # Using a dynamic DNS resolver - domain: $(SPACE_NAME).$(CLUSTER_INGRESS_IP).nip.io # Creating an internal domain only visible within the cluster - domain: $(SPACE_NAME)-apps.internal gatewayName: kf/internal-gateway Buildpacks V2 lifecycle builder The buildpacksV2LifecycleBuilder property contains the version of the Cloud Foundry builder binary used execute buildpack v2 builds.\nThe value is a Git reference. To use a specific version, append an @ symbol followed by a Git SHA to the end.\nExample:\nbuildpacksV2LifecycleBuilder: \"code.cloudfoundry.org/buildpackapplifecycle/builder@GIT_SHA\" Buildpacks V2 lifecycle launcher The buildpacksV2LifecycleLauncher property contains the version of the Cloud Foundry launcher binary built into every buildpack V2 application.\nThe value is a Git reference. To use a specific version, append an @ symbol followed by a Git SHA to the end.\nExample:\nbuildpacksV2LifecycleLauncher: \"code.cloudfoundry.org/buildpackapplifecycle/launcher@GIT_SHA\" Buildpacks V2 list The spaceBuildpacksV2 property is a string encoded YAML array that holds an ordered list of default buildpacks that are used to build applications compatible with the V2 buildpacks process.\nFields name string\nA short name developers can use to reference the buildpack by in their application manifests.\nurl string\nThe URL used to fetch the buildpack.\ndisabled boolean\nUsed to prevent this buildpack from executing.\nStacks V2 list The spaceBuildpacksV2 property is a string encoded YAML array that holds an ordered list of stacks that can be used with Cloud Foundry compatible builds.\nFields name string\nA short name developers can use to reference the stack by in their application manifests.\nimage string\nURL of the container image to use as the stack. For more information, see https://kubernetes.io/docs/concepts/containers/images. Note: Use images tagged with SHAs to improve caching. Stacks V3 list The spaceStacksV3 property is a string encoded YAML array that holds an ordered list of stacks that can be used with Cloud Native Buildpack builds.\nFields name string\nA short name developers can use to reference the stack by in their application manifests.\ndescription string\nA short description of the stack shown when running kf stacks.\nbuildImage string\nURL of the container image to use as the builder. For more information, see https://kubernetes.io/docs/concepts/containers/images. runImage string\nURL of the container image to use as the base for all apps built with . For more information, see https://kubernetes.io/docs/concepts/containers/images. nodeSelector map (key: string, value: string)\n(Optional)\nA NodeSelector used to indicate which nodes applications built with this stack can run on. Note: Use images tagged with SHAs to improve caching. Example:\nspaceStacksV3: | - name: heroku-18 description: The official Heroku stack based on Ubuntu 18.04 buildImage: heroku/pack:18-build runImage: heroku/pack:18 nodeSelector: kubernetes.io/os: windows Default to V3 Stack The spaceDefaultToV3Stack property contains a quoted value true or false indicating whether spaces should use V3 stacks if a user doesn’t specify one.\nFeature flags The featureFlags property contains a string encoded YAML map of feature flags that can enable and disable features of Kf.\nFlag names that aren’t supported by Kf will be ignored.\nFlag Name Default Purpose disable_custom_builds false Disable developer access to arbitrary Tekton build pipelines. enable_dockerfile_builds true Allow developers to build source code from dockerfiles. enable_custom_buildpacks true Allow developers to specify external buildpacks in their applications. enable_custom_stacks true Allow developers to specify custom stacks in their applications. Example:\nfeatureFlags: | disable_custom_builds: false enable_dockerfile_builds: true enable_some_feature: true ","categories":"","description":"Learn to configure your Kf cluster's settings.","excerpt":"Learn to configure your Kf cluster's settings.","ref":"/kf/docs/v2.11/operator/customizing/cluster-configuration/","tags":"","title":"Cluster Configuration"},{"body":"Kf supports a variety of buildpacks. This document covers some starter examples for using them.\nBefore you begin You should have Kf running on a cluster. You should have run kf target -s \u003cspace-name\u003e to target your space. Java (v2) buildpack Use spring initializr to create a Java 8 maven project with a spring web dependency and JAR packaging. Download it, extract it, and once extracted you can generate a JAR.\n./mvnw package Push the JAR to Kf with the Java v2 buildpack.\nkf push java-v2 --path target/helloworld-0.0.1-SNAPSHOT.jar Java (v3) buildpack Use spring initializr to create a Java 8 maven project with a spring web dependency and JAR packaging. Download it, extract it, and once extracted, push to Kf with the cloud native buildpack.\nkf push java-v3 --stack org.cloudfoundry.stacks.cflinuxfs3 Python (v2) buildpack Create a new directory with files as shown in the following structure.\ntree . ├── Procfile ├── requirements.txt └── server.py cat Procfile web: python server.py cat requirements.txt Flask cat server.py from flask import Flask import os app = Flask(__name__) @app.route('/') def hello_world(): return 'Hello, World!' if __name__ == \"__main__\": port = int(os.getenv(\"PORT\", 8080)) app.run(host='0.0.0.0', port=port) Push the Python flask app using v2 buildpacks.\nkf push python --buildpack python\\_buildpack Python (v3) buildpack (same as above)\nPush the Python flask app using cloud native buildpacks.\nkf push pythonv3 --stack org.cloudfoundry.stacks.cflinuxfs3 Staticfile (v2) buildpack Create a new directory that holds your source code.\nAdd an index.html file with this content.\n\u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e\u003ctitle\u003eHello, world!\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e\u003ch1\u003eHello, world!\u003c/h1\u003e\u003c/body\u003e \u003c/html\u003e Push the static content with the staticfile buildpack.\nkf push staticsite --buildpack staticfile\\_buildpack ","categories":"","description":"How to use built-in buildpacks for various languages.","excerpt":"How to use built-in buildpacks for various languages.","ref":"/kf/docs/v2.11/developer/build-and-deploy/buildpacks-getting-started/","tags":"","title":"Get started with buildpacks"},{"body":"Kf provides a set of Kubernetes roles that allow multiple teams to share a Kf cluster. This page describes the roles and best practices to follow when using them.\nWhen to use Kf roles Kf roles allow multiple teams to share a Kubernetes cluster with Kf installed. The roles provide access to individual Kf Spaces.\nUse Kf roles to share access to a cluster if the following are true:\nThe cluster is used by trusted teams. Workloads on the cluster share the same assumptions about the level of security provided by the environment. The cluster exists in a Google Cloud project that is tightly controlled. Kf roles will not:\nProtect your cluster from untrusted developers or workloads. See the GKE shared responsibility model for more information. Provide isolation for your workloads. See the guide to harden your cluster for more information. Prevent additional Kubernetes roles from being defined that interact with Kf. Prevent access from administrators who have access to the Google Cloud project or cluster. Kf roles The following sections describe the Kubernetes RBAC Roles provided by Kf and how they interact with GKE IAM.\nPredefined roles Kf provides several predefined Kubernetes roles to help you provide access to different subjects that perform different roles. Each predefined role can be bound to a subject within a Kubernetes Namespace managed by a Kf Space.\nWhen a subject is bound to a role within a Kubernetes Namespace, their access is limited to objects that exist in the Namespace that match the grants listed in the role. In Kf, some resources are defined at the cluster scope. Kf watches for changes to subjects in the Namespace and grants additional, limited, roles at the cluster scope.\nRole Title Description Scope space-auditor Space Auditor Allows read-only access to a Space. Space space-developer Space Developer Allows application developers to deploy and manage applications within a Space. Space space-manager Space Manager Allows administration and the ability to manage auditors, developers, and managers in a Space. Space SPACE_NAME-manager Dynamic Space Manager Provides write access to a single Space object, automatically granted to all subjects with the space-manager role within the named Space. Cluster kf-cluster-reader Cluster Reader Allows read-only access to cluster-scoped Kf objects, automatically granted to all space-auditor, space-developer, and space-manager. Cluster Information about the policy rules that make up each predefined role can be found in the Kf roles reference documentation.\nGoogle Cloud IAM roles Kf roles provide access control for objects within a Kubernetes cluster. Subjects must also be granted an Cloud IAM role to authenticate to the cluster:\nPlatform administrators should be granted the roles/container.admin Cloud IAM role. This will allow them to install, upgrade, and delete Kf as well as create, and delete cluster scoped Kf objects like Spaces or ClusterServiceBrokers.\nKf end-users should be granted the roles/container.viewer Cloud IAM role. This role will allow them to authenticate to a cluster with limited permissions that can be expanded using Kf roles.\nGoogle Cloud IAM offers additional predefined Roles for GKE to solve more advanced use cases.\nMap Cloud Foundry roles to Kf Cloud Foundry provides roles are similar to Kf’s predefined roles. Cloud Foundry has two major types of roles:\nRoles assigned by the User Account and Authentication (UAA) subsystem that provide coarse-grained OAuth scopes applicable to all Cloud Foundry API endpoints. Roles granted within the Cloud Controller API (CAPI) that provide fine-grained access to API resources. UAA roles Roles provided by UAA are most similar to project scoped Google Cloud IAM roles:\nAdmin users in Cloud Foundry can perform administrative activities for all Cloud Foundry organizations and spaces. The role is most similar to the roles/container.admin Google Cloud IAM role. Admin read-only users in Cloud Foundry can access all Cloud Foundry API endpoints. The role is most similar to the roles/container.admin Google Cloud IAM role. Global auditor users in Cloud Foundry have read access to all Cloud Foundry API endpoints except for secrets. There is no equivalent Google Cloud IAM role, but you can create a custom role with similar permissions. Cloud Controller API roles Roles provided by CAPI are most similar to Kf roles granted within a cluster to subjects that have the roles/container.viewer Google Cloud IAM role on the owning project:\nSpace auditors in Cloud Foundry have read-access to resources in a CF space. The role is most similar to the space-auditor Kf role. Space developers in Cloud Foundry have the ability to deploy and manage applications in a CF space. The role is most similar to the space-developer Kf role. Space managers in Cloud Foundry can modify settings for the CF space and assign users to roles. The role is most similar to the space-manager Kf role. What’s next Learn more about GKE security in the Security Overview. Make sure you understand the GKE shared responsibility model. Learn more about access control in GKE. Read the GKE multi-tenancy overview. Learn about hardening your GKE cluster. Understand the Kubernetes permissions that make up each Kf predefined role. ","categories":"","description":"Learn about how to share a Kf cluster using roles.","excerpt":"Learn about how to share a Kf cluster using roles.","ref":"/kf/docs/v2.11/operator/security/role-based-access-control/","tags":"","title":"Role-based access control"},{"body":"You can execute short-lived workflows by running them as Tasks. Running Tasks describes how to run Tasks under Apps.\nYou can also schedule Tasks to run at recurring intervals specified using the unix-cron format. With scheduled Tasks, you first push an App running the Task as you do with an unscheduled Task, and then create a Job to schedule the Task.\nYou can define a schedule so that your Task runs multiple times a day or on specific days and months.\nPush an App for running scheduled Tasks Clone the test-app repo: git clone https://github.com/cloudfoundry-samples/test-app test-app cd test-app Push the App.\nPush the App with the kf push APP_NAME --task command. The --task flag indicates that the App is meant to be used for running Tasks, and thus no routes will be created on the App and it will not be deployed as a long-running application.\nkf push test-app --task Confirm that no App instances or routes were created by listing the App:\nkf apps Notice that the App is not started and has no URLs:\nListing Apps in Space: test-space Name Instances Memory Disk CPU URLs test-app stopped 1Gi 1Gi 100m \u003cnil\u003e Create a Job To run a Task on a schedule, you must first create a Job that describes the Task:\nkf create-job test-app test-job \"printenv\" The Job starts suspended or unscheduled, and does not create Tasks until it is manually executed by kf run-job or scheduled by kf schedule-task.\nRun a Job manually Jobs can be run ad hoc similar to running Tasks by kf run-task. This option can be useful for testing the Job before scheduling or running as needed in addition to the schedule.\nkf run-job test-job This command runs the Task defined by the Job a single time immediately.\nSchedule a Job To schedule the Job for execution, you must provide a unix-cron schedule in the kf schedule-job command:\nkf schedule-job test-job \"* * * * *\" This command triggers the Job to automatically create Tasks on the specified schedule. In this example a Task runs every minute.\nYou can update a Job’s schedule by running kf schedule-task with a new schedule. Jobs in Kf can only have a single cron schedule. This differs from the PCF Scheduler, which allows multiple schedules for a single Job. If you require multiple cron schedules, then you can achieve that with multiple Jobs.\nManage Jobs and schedules View all Jobs, both scheduled and unscheduled, in the current Space by using the kf jobs command:\n$ kf jobs Listing Jobs in Space: test space Name Schedule Suspend LastSchedule Age Ready Reason test-job * * * * * \u003cnil\u003e 16s 2m True \u003cnil\u003e unscheduled-job 0 0 30 2 * true 16s 2m True \u003cnil\u003e Note: The unscheduled-job has a default schedule set (0 0 30 2 *). This schedule is not active because the Job is suspended and is only present as a placeholder schedule for unscheduled Jobs. Additionally, you can view only Jobs that are actively scheduled with the kf job-schedules command.\n$ kf job-schedules Listing job schedules in Space: test space Name Schedule Suspend LastSchedule Age Ready Reason test-job * * * * * \u003cnil\u003e 16s 2m True \u003cnil\u003e Notice how the unscheduled-job is not listed in the kf job-schedules output.\nCancel a Job’s schedule You can stop a scheduled Job with the kf delete-job-schedule command:\nkf delete-job-schedule test-job This command suspends the Job and stops it from creating Tasks on the previous schedule. The Job is not deleted and can be scheduled again by kf schedule-job to continue execution.\nDelete a Job The entire Job can be deleted with the kf delete-job command:\nkf delete-job test-job This command deletes the Job and all Tasks that were created by the Job, both scheduled and manual executions. If any Tasks are still running, this command forcefully deletes them.\nIf you want to ensure that running Tasks are not interrupted, then first delete the Jobs schedule with kf delete-job-schedule, wait for all Tasks to complete, and then delete the job by calling kf delete-job.\n","categories":"","description":"Learn how to schedule tasks to run periodic jobs.\n","excerpt":"Learn how to schedule tasks to run periodic jobs.\n","ref":"/kf/docs/v2.11/developer/tasks/scheduling-tasks/","tags":"","title":"Schedule Tasks"},{"body":" Warning: Changes will be instantly pushed to all Kf resources, test any customization options before using them in production. Build Retention You can control how many Kf Builds are kept before being garbage collected.\nNote: This example sets the retention to 1 Build. Change the value as needed. kubectl patch \\ kfsystem kfsystem \\ --type='json' \\ -p=\"[{'op': 'replace', 'path': '/spec/kf/config/buildRetentionCount', 'value': 1}]\" Enable or Disable the Istio Sidecar If you do not require the Istio sidecar for the Build pods, then they can be disabled by setting the value to true. Enable by setting the value to false.\nkubectl patch \\ kfsystem kfsystem \\ --type='json' \\ -p=\"[{'op': 'replace', 'path': '/spec/kf/config/buildDisableIstioSidecar', 'value': true}]\" Build Pod Resource Limits The default pod resource size can be increased from the default to accommodate very large builds. The units for the value are in Mi or Gi.\nNote: This is only applicable for built-in Tasks (which is normal for a kf push build). For V2 buildpack builds, this will be set on two steps and one for V3 buildpacks or Dockerfiles. This means that for a V2 build the required Pod size will be double the limit. For example, if the memory limit is 1Gi, then the pod will require 2Gi. kubectl patch \\ kfsystem kfsystem \\ --type='json' \\ -p=\"[{'op': 'replace', 'path': '/spec/kf/config/buildPodResources', 'value': {'limits': {'memory': '234Mi'}}}]\" Read Kubernetes container resource docs for more information about container resource management.\nSelf Signed Certificates for Service Brokers If you want to use self signed certificates for TLS (https instead of http) for the service broker URL, the Kf controller requires the CA certificate. To configure Kf for this scenario, create an immutable Kubernetes secret in the kf namespace and update the kfsystem.spec.kf.config.secrets.controllerCACerts.name object to point to it.\nCreate a secret to store the self-signed certificate.\nNote: Customize the secret name if desired, or leave the default name of cacerts. Replace /path/to/cert/certs.pem with the path to the self-signed certificate. kubectl create secret generic cacerts -nkf --from-file /path/to/cert/certs.pem Make the secret immutable.\nkubectl patch -nkf secret cacerts \\ --type='json' \\ -p=\"[{'op':'add','path':'/immutable','value':true}]\" Update kfsystem to point to the secret.\nNote: This will cause the controller pod to be re-deployed with the certs mounted as a volume. kubectl patch \\ kfsystem kfsystem \\ --type='json' \\ -p=\"[{'op':'add','path':'/spec/kf/config/secrets','value':{'controllerCACerts':{'name':'\u003cvar\u003ecacerts\u003c/var\u003e'}}}]\" ","categories":"","description":"Learn to configure your Kf cluster's settings.","excerpt":"Learn to configure your Kf cluster's settings.","ref":"/kf/docs/v2.11/operator/customizing/customizing-features/","tags":"","title":"Customizing Kf Features"},{"body":" Note: Kf will perform rolling deployments by default if you upgrade an app in place by starting an extra instance of it, waiting for it to become healthy, and then replacing an existing instance. This page shows you how to deploy a new version of your application and migrate traffic over from an old to a new version.\nPush the initial App Use the Kf CLI to push the initial version of your App with any routes:\n$ kf push app-v1 --route my-app.my-space.example.com Push the updated App Use the Kf CLI to push a new version of your App without any routes:\n$ kf push app-v2 --no-route Add routes to the updated App Use the Kf CLI to bind all existing routes to the updated App with a weight of 0 to ensure that they don’t get any requests:\n$ kf map-route app-v2 my-space.example.com --hostname my-app --weight 0 Shift traffic Start shifting traffic from the old App to the updated App by updating the weights on the routes:\n$ kf map-route app-v1 my-space.example.com --hostname my-app --weight 80 $ kf map-route app-v2 my-space.example.com --hostname my-app --weight 20 If the deployment is going well, you can shift more traffic by updating the weights again:\n$ kf map-route app-v1 my-space.example.com --hostname my-app --weight 50 $ kf map-route app-v2 my-space.example.com --hostname my-app --weight 50 Note: Weights split traffic proportionally to the sum of the weights of all Apps mapped to the route. It’s a common practice to treat weights as percentages. Complete traffic shifting After you’re satisfied that the new service hasn’t introduced regressions, complete the rollout by shifting all traffic to the new instance:\n$ kf map-route app-v1 my-space.example.com --hostname my-app --weight 0 $ kf map-route app-v2 my-space.example.com --hostname my-app --weight 100 Turn down the original App After you’re satisfied that quick rollbacks aren’t needed, remove the original route and stop the App:\n$ kf unmap-route app-v1 myspace.example.com --hostname my-app $ kf stop app-v1 Or delete the App and all associated route mappings:\n$ kf delete app-v1 ","categories":"","description":"How to deploy highly available applications.","excerpt":"How to deploy highly available applications.","ref":"/kf/docs/v2.11/developer/build-and-deploy/howto-blue-green/","tags":"","title":"Reduce deployment risk with blue-green deployments"},{"body":"A buildpack converts source code into an executable, and is used to deliver a simple, reliable, and repeatable way to create containers. Kf supports both V2 and V3 buildpacks, and it is important to understand the differences between them.\nV2 buildpacks Most Cloud Foundry applications already use V2 buildpacks. When using V2 buildpacks with Kf, the lifecycle binaries and the buildpacks are downloaded and configured from their git URLs. Kf then uses the lifecycle CLI to execute each buildpack against the source code.\nPros Ready out of the box without pipeline or code changes. Cons Legacy buildpack supersceded by V3. Weaker performance and reliability. The Kf build pipeline requires more IO for V2 buildpacks. Fewer community resources. Kf only supports OSS git repos. V3 buildpacks V3 buildpacks are a Cloud Native Computing Foundation (CNCF) project with a well defined spec, a CLI (pack) and a growing community that is innovating around different languages and frameworks. Google Cloud also has its own set of OSS buildpacks.\nV3 buildpacks have two overarching OCI containers:\nBuilder image Run image Builder image The builder image is used while your source code is being built into a runnable container. The image has the necessary detect scripts and other utilities to compile source code.\nNote: Most buildpacks don’t package the compilers with the builder image. Instead, they may have the build pipelines download any dependencies required to compile. Run image The run image is the base image that a container is built on. This means that it is the base image that will run when the App executes.\nLayers V3 buildpacks use layers to compose the final container. Each buildpack included in a build is given the opportunity to manipulate the file system and environment variables of the App. This layering approach allows for buildpacks to be thinner and more generic.\nV3 buildpacks are built on OCI containers. This requires that the V3 builder image be stored in a container registry that the Kf build pipeline has access to. The build pipeline uses the builder image to apply the underlying scripts to build the source code into a runnable container.\nPros Google supported builder and run image. Works with various CI/CD runtimes like Cloud Build. Growing community and buildpack registry. Cons May require code/process updates. For example, the Java buildpack requires source code while the V2 buildpack requires a jar file. V3 buildpacks are newer and might require additional validation (is using community developed buildpacks). Kf Stacks View Stacks When pushing an App, the build pipeline determines the buildpack based on the selected Stack (specified via the --stack flag or the manifest).\nTo see which Stacks are available in a Space first ensure a Space is targeted:\nkf target -s myspace The kf stacks subcommand can then be used to list the Stacks:\nkf stacks The output shows both V2 and V3 Stacks:\nGetting stacks in Space: myspace Version Name Build Image Run Image V2 cflinuxfs3 cloudfoundry/cflinuxfs3@sha256:5219e9e30000e43e5da17906581127b38fa6417f297f522e332a801e737928f5 cloudfoundry/cflinuxfs3@sha256:5219e9e30000e43e5da17906581127b38fa6417f297f522e332a801e737928f5 V3 kf-v2-to-v3-shim gcr.io/kf-releases/v2-to-v3:v2.7.0 gcr.io/buildpacks/gcp/run:v1 This is a stack added by the integration tests to assert that v2-\u003ev3 shim works V3 google gcr.io/buildpacks/builder:v1 gcr.io/buildpacks/gcp/run:v1 Google buildpacks (https://github.com/GoogleCloudPlatform/buildpacks) V3 org.cloudfoundry.stacks.cflinuxfs3 cloudfoundry/cnb:cflinuxfs3@sha256:f96b6e3528185368dd6af1d9657527437cefdaa5fa135338462f68f9c9db3022 cloudfoundry/run:full-cnb@sha256:dbe17be507b1cc6ffae1e9edf02806fe0e28ffbbb89a6c7ef41f37b69156c3c2 A large Cloud Foundry stack based on Ubuntu 18.04 V2 to V3 Buildpack Migration Note: This feature is currently experimental and subject to change. Kf provides a V3 stack to build applications that were built with standard V2 buildpacks, using a stack named kf-v2-to-v3-shim. The kf-v2-to-v3-shim stack is created following the standard V3 buildpacks API. A Google maintained builder image is created with each Kf release, following the standard buildpack process. The builder image aggregates a list of V3 buildpacks created by the same process used with the kf wrap-v2-buildpack command. The V3 buildpack images are created using the standard V2 buildpack images. It’s important to note that the V3 buildpacks do not contain the binary of the referenced V2 buildpacks. Instead, the V2 buildpack images are referenced, and the bits are downloaded at App build time (by running kf push).\nAt App build time, the V2 buildpack is downloaded from the corresponding git repository. When V3 detection runs, it delegates to the downloaded V2 detection script. For the first buildpack group that passes detection, it proceeds to the build step, which delegates the build execution to the downloaded V2 builder script.\nThe following V2 buildpacks are supported in the kf-v2-to-v3-shim stack:\nBuildpack Git Repository java_buildpack https://github.com/cloudfoundry/java-buildpack dotnet_core_buildpack https://github.com/cloudfoundry/dotnet-core-buildpack nodejs_buildpack https://github.com/cloudfoundry/nodejs-buildpack go_buildpack https://github.com/cloudfoundry/go-buildpack python_buildpack https://github.com/cloudfoundry/python-buildpack binary_buildpack https://github.com/cloudfoundry/binary-buildpack nginx_buildpack https://github.com/cloudfoundry/nginx-buildpack Option 1: Migrate Apps built with standard V2 buildpacks To build Apps with the kf-v2-to-v3-shim stack, use the following command:\nkf push myapp --stack kf-v2-to-v3-shim The kf-v2-to-v3-shim stack will automatically detect the runtime with the wrapped V2 buildpacks. The resulting App image is created using the V3 standard and build pipeline, but the builder of the equivalent V2 buildpack.\nOption 2: Migrate Apps built with custom V2 buildpacks Kf has a buildpack migration tool that can take a V2 buildpack and wrap it with a V3 buildpack. The wrapped buildpack can then be used anywhere V3 buildpacks are available.\nkf wrap-v2-buildpack gcr.io/your-project/v2-go-buildpack https://github.com/cloudfoundry/go-buildpack --publish This will create a buildpack image named gcr.io/your-project/v2-go-buildpack. It can then be used to create a builder by following the create a builder docs.\nThis subcommand uses the following CLIs transparently:\ngo git pack unzip ","categories":"","description":"Learn how to decide wihch buildpacks to use.\n","excerpt":"Learn how to decide wihch buildpacks to use.\n","ref":"/kf/docs/v2.11/developer/build-and-deploy/v2-vs-v3-buildpacks/","tags":"","title":"Compare V2 and V3 Buildpacks"},{"body":"","categories":"","description":"Learn to run and maintain Kf as a platform.","excerpt":"Learn to run and maintain Kf as a platform.","ref":"/kf/docs/v2.11/operator/","tags":"","title":"Operate and Maintain Kf"},{"body":"","categories":"","description":"Troubleshoot Kf resources or installation.","excerpt":"Troubleshoot Kf resources or installation.","ref":"/kf/docs/v2.11/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Exapmles and demonstrations of special use-cases.","excerpt":"Exapmles and demonstrations of special use-cases.","ref":"/kf/docs/v2.11/examples/","tags":"","title":"Examples"},{"body":"The Kf Cloud Service Broker is a Service Broker bundle that includes the open source Cloud Service Broker and Google Cloud Brokerpak. It is made available as a public Docker image and ready to deploy as a Kubernetes service in Kf clusters. Once the Kf Cloud Service Broker service is deployed in a cluster, developers can provision Google Cloud backing services through the Kf Cloud Service Broker service, and bind the backing services to Kf Apps.\nNote: Kf Cloud Service Brokeris not customizable, and the default Google Cloud Brokerpak is included. If you would like to use a custom Brokerpak, you can follow the steps in the open source Cloud Service Broker Google Cloud installation guide. Requirements Kf Cloud Service Broker requires a MySQL instance and a service account for accessing the MySQL instance and Google Cloud backing services to be provisioned. Connection from the Kf Cloud Service Broker to the MySQL instance goes through the Cloud SQL Auth Proxy. Requests to access Google Cloud services (for example: Cloud SQL for MySQL or Cloud Memorystore for Redis) are authenticated via Workload Identity. Override Brokerpak defaults Brokerpaks are essentially a Terraform plan and related dependencies in a tar file. You can inspect the Terraform plans to see what the defaults are, and then you can tell Kf Cloud Service Broker to override them when creating new services.\nFor example, the Terraform plan for MySQL includes a variable called authorized_network. If not overridden, the default VPC will be used. If you’d like to override the default, you can pass that during service creation. Here are some examples:\nOverride the compute region config. kf create-service csb-google-postgres small spring-music-postgres-db -c '{\"config\":\"YOUR_COMPUTE_REGION\"}' Override the authorized_network and compute region config. kf create-service csb-google-postgres small spring-music-postgres-db -c '{\"config\":\"YOUR_COMPUTE_REGION\",\"authorized_network\":\"YOUR_CUSTOM_VPC_NAME\"}' You can learn more by reading the MySQL Plans and Configs documentation.\nArchitecture The following Kf Cloud Service Broker architecture shows how instances are created.\nThe Kf Cloud Service Broker (CSB) is installed in its own namespace. On installation, a MySQL instance must be provided to persist business logic used by Kf Cloud Service Broker. Requests are sent securely from the Kf Cloud Service Broker pod to the MySQL instance via the MySQL Auth Proxy. On service provisioning, a Kf Service custom resource is created. The reconciler of the Kf Service provisions Google Cloud backing services using the Open Service Broker API. When a request to provision/deprovision backing resources is received, Kf Cloud Service Broker sends resource creation/deletion requests to the corresponding Google Cloud service, and these requests are authenticated with Workload Identity. It also persists the business logics (e.g. mapping of Kf services to backing services, service bindings) to the MySQL instance. On backing service creation success, the backing service is bound to an App via VCAP_SERVICES. What’s next? Deploy Kf Cloud Service Broker. Learn how to list and provision services. {% endblock %}\n","categories":"","description":"","excerpt":"The Kf Cloud Service Broker is a Service Broker bundle that includes …","ref":"/kf/docs/v2.11/operator/service-brokers/cloud-sb-overview/","tags":"","title":"About Kf Cloud Service Broker"},{"body":"App manifests provide a way for developers to record their App’s execution environment in a declarative way. They allow Apps to be deployed consistently and reproducibly.\nFormat Manifests are YAML files in the root directory of the App. They must be named manifest.yml or manifest.yaml.\nKf App manifests are allowed to have a single top-level element: applications. The applications element can contain one or more application entries.\nApplication fields The following fields are valid for objects under applications:\nField Type Description name string The name of the application. The app name should be lower-case alphanumeric characters and dashes. It must not start with a dash. path string The path to the source of the app. Defaults to the manifest’s directory. buildpacks string[] A list of buildpacks to apply to the app. stack string Base image to use for to use for apps created with a buildpack. docker object A docker object. See the Docker Fields section for more information. env map Key/value pairs to use as the environment variables for the app and build. services string[] A list of service instance names to automatically bind to the app. disk_quota quantity The amount of disk the application should get. Defaults to 1GiB. memory quantity The amount of RAM to provide the app. Defaults to 1GiB. cpu † quantity The amount of CPU to provide the application. Defaults to 0.1 (1/10th of a CPU). instances int The number of instances of the app to run. Defaults to 1. routes object A list of routes the app should listen on. See the Route Fields section for more. no-route boolean If set to true, the application will not be routable. random-route boolean If set to true, the app will be given a random route. timeout int The number of seconds to wait for the app to become healthy. health-check-type string The type of health-check to use port, process, none, or http. Default: port health-check-http-endpoint string The endpoint to target as part of the health-check. Only valid if health-check-type is http. command string The command that starts the app. If supplied, this will be passed to the container entrypoint. entrypoint † string Overrides the app container’s entrypoint. args † string[] Overrides the arguments the app container. ports † object A list of ports to expose on the container. If supplied, the first entry in this list is used as the default port. † Unique to Kf\nDocker fields The following fields are valid for application.docker objects:\nField Type Description image string The docker image to use. Route fields The following fields are valid for application.routes objects:\nField Type Description route string A route to the app including hostname, domain, and path. appPort int (Optional) A custom port on the App the route will send traffic to. Note: If you specify an appPort, that port MUST also be declared in the ports field. Port fields The following fields are valid for application.ports objects:\nField Type Description port int The port to expose on the App’s container. protocol string The protocol of the port to expose. Must be tcp, http or http2. Default: tcp Note: The protocol field is a hint about the traffic that goes over the port. The hint is used by the service mesh for better tracing and metrics. Warning: Kf doesn’t currently support TCP port-based routing. You must use a Kubernetes LoadBalancer if you want to expose a TCP port to the Internet. Ports are available on the cluster internal App address \u003capp-name\u003e.\u003cspace\u003e. Examples Minimal App This is a bare-bones manifest that will build an App by auto-detecting the buildpack based on the uploaded source, and deploy one instance of it.\n--- applications: - name: my-minimal-application Simple App This is a full manifest for a more traditional Java App.\n--- applications: - name: account-manager # only upload src/ on push path: src # use the Java buildpack buildpacks: - java env: # manually configure the buildpack's Java version BP_JAVA_VERSION: 8 ENVIRONMENT: PRODUCTION # use less disk and memory than default disk_quota: 512M memory: 512M # bump up the CPU cpu: 0.2 instances: 3 # make the app listen on three routes routes: - route: accounts.mycompany.com - route: accounts.datacenter.mycompany.internal - route: mycompany.com/accounts # set up a longer timeout and custom endpoint to validate # when the app comes up timeout: 300 health-check-type: http health-check-http-endpoint: /healthz # attach two services by name services: - customer-database - web-cache Docker App Kf can deploy Docker containers as well as manifest deployed App. These Docker Apps MUST listen on the PORT environment variable.\n--- applications: - name: white-label-app # use a pre-built docker image (must listen on $PORT) docker: image: gcr.io/my-company/white-label-app:123 env: # add additional environment variables ENVIRONMENT: PRODUCTION disk_quota: 1G memory: 1G cpu: 2 instances: 1 routes: - route: white-label-app.mycompany.com App with multiple ports This App has multiple ports to expose an admin console, website, and SMTP server.\n--- applications: - name: b2b-server ports: - port: 8080 protocol: http - port: 9090 protocol: http - port: 2525 protocol: tcp routes: - route: b2b-admin.mycompany.com appPort: 9090 - route: b2b.mycompany.com # gets the default (first) port Health check types Kf supports three different health check types:\nport (default) http process (or none) port and http set a Kubernetes readiness and liveness probe that ensures the application is ready before sending traffic to it.\nThe port health check will ensure the port found at $PORT is being listened to. Under the hood Kf uses a TCP probe.\nThe http health check will use the configured value in health-check-http-endpoint to check the application’s health. Under the hood Kf uses an HTTP probe.\nA process health check only checks to see if the process running on the container is alive. It does NOT set a Kubernetes readiness or liveness probe.\nKnown differences The following are known differences between Kf manifests and CF manifests:\nKf does not support deprecated CF manifest fields. This includes all fields at the root-level of the manifest (other than applications) and routing fields. Kf is missing support for the following v2 manifest fields: docker.username Kf does not support auto-detecting ports for Docker containers. ","categories":"","description":"Reference guide for the application manifest.yml format.","excerpt":"Reference guide for the application manifest.yml format.","ref":"/kf/docs/v2.11/developer/build-and-deploy/manifest/","tags":"","title":"App Manifest"},{"body":"The app runtime is the environment apps are executed in.\nBuildpack Apps Container Image Apps System libraries Provided by the Stack Provided in the container Network access Full access through Envoy sidecar Full access through Envoy sidecar File system Ephemeral storage Ephemeral storage Language runtime Supplied by the Stack or Buildpack Built into the container User Specified by the Stack Specified on the container Isolation mechanism Kubernetes Pod Kubernetes Pod DNS Provided by Kubernetes Provided by Kubernetes Environment variables Environment variables are injected into the app at runtime by Kubernetes. Variables are added based on the following order, where later values override earlier ones with the same name:\nSpace (set by administrators) App (set by developers) System (set by Kf) Kf provides the following system environment variables:\nVariable Purpose CF_INSTANCE_ADDR The cluster-visible IP:PORT of the App instance. CF_INSTANCE_GUID The UUID of the App instance. INSTANCE_GUID Alias of CF_INSTANCE_GUID. CF_INSTANCE_INDEX The index number of the App instance, this will ALWAYS be 0. INSTANCE_INDEX Alias of CF_INSTANCE_INDEX. CF_INSTANCE_IP The cluster-visible IP of the App instance. CF_INSTANCE_INTERNAL_IP Alias of CF_INSTANCE_IP VCAP_APP_HOST Alias of CF_INSTANCE_IP CF_INSTANCE_PORT The cluster-visible port of the App instance. In Kf this is the same as PORT. DATABASE_URL The first URI found in a VCAP_SERVICES credential. LANG Required by Buildpacks to ensure consistent script load order. MEMORY_LIMIT The maximum amount of memory in MB the App can consume. PORT The port the App should listen on for requests. VCAP_APP_PORT Alias of PORT. VCAP_APPLICATION A JSON structure containing App metadata. VCAP_SERVICES A JSON structure specifying bound services. Service credentials from bound services get injected into Apps via the VCAP_SERVICES environment variable. The variable is a valid JSON object with the following structure.\nVCAPServices A JSON object where the keys are Service labels and the values are an array of VCAPService. The array represents every bound service with that label. User provided services are placed under the label user-provided.\nExample\n{ \"mysql\": [...], \"postgresql\": [...], \"user-provided\": [...] } VCAPService This type represents a single bound service instance.\nExample\n{ \"binding_name\": string, \"instance_name\": string, \"name\": string, \"label\": string, \"tags\": string[], \"plan\": string, \"credentials\": object } Fields\nField Type Description binding_name string The name assigned to the service binding by the user. instance_name string The name assigned to the service instance by the user. name string The binding_name if it exists; otherwise the instance_name. label string The name of the service offering. tags string[] An array of strings an app can use to identify a service instance. plan string[] The service plan selected when the service instance was created. credentials object The service-specific credentials needed to access the service instance. Note: Use the tags field to discover services in your App rather than filtering by plan/label so you can swap implementations and providers per environment. Note: The credentials field is often an object with string keys and values. However, the values are allowed to be any type. VCAP_APPLICATION TheVCAP_APPLICATION environment variable is a JSON object containing metadata about the App.\nExample\n{ \"application_id\": \"12345\", \"application_name\": \"my-app\", \"application_uris\": [\"my-app.example.com\"], \"limits\": { \"disk\": 1024, \"mem\": 256 }, \"name\": \"my-app\", \"process_id\": \"12345\", \"process_type\": \"web\", \"space_name\": \"my-ns\", \"uris\": [\"my-app.example.com\"] } Fields\nField Type Description application_id string The GUID identifying the App. application_name string The name assigned to the App when it was pushed. application_uris string[] The URIs assigned to the App. limits object The limits to disk space, and memory permitted to the App. Memory and disk space limits are supplied when the App is deployed, either on the command line or in the App manifest. Disk and memory limits are represented as integers, with an assumed unit of MB. name string Identical to application_name. process_id string The UID identifying the process. Only present in running App containers. process_type string The type of process. Only present in running App containers. space_name string The human-readable name of the Space where the App is deployed. uris string[] Identical to application_uris. Missing Fields\nSome fields in VCAP_APPLICATION that are in Cloud Foundry are currently not supported in Kf.\nBesides CF-specific and deprecated fields (cf_api, host, users) the fields that are not supported in Kf are:\napplication_version (identical to version) organization_id organization_name space_id start (identical to started_at) started_at_timestamp (identical to state_timestamp) ","categories":"","description":"Reference guide for the application runtime container environment.","excerpt":"Reference guide for the application runtime container environment.","ref":"/kf/docs/v2.11/developer/build-and-deploy/app-runtime/","tags":"","title":"App runtime"},{"body":"The Build runtime is the environment Apps are built in.\nBuildpack Builds Docker Builds System libraries Provided by the Stack User supplied Network access Full access through Envoy sidecar Full access through Envoy sidecar File system No storage No storage Language runtime Provided by the Stack User supplied User Specified by the Stack User supplied Isolation mechanism Kubernetes Pod Kubernetes Pod DNS Provided by Kubernetes Provided by Kubernetes Environment variables Environment variables are injected into the Build at runtime. Variables are added based on the following order, where later values override earlier ones with the same name:\nSpace (set by administrators) App (set by developers) System (set by Kf) Kf provides the following system environment variables to Builds:\nVariable Purpose CF_INSTANCE_ADDR The cluster-visible IP:PORT of the Build. INSTANCE_GUID Alias of CF_INSTANCE_GUID. CF_INSTANCE_IP The cluster-visible IP of the Build. CF_INSTANCE_INTERNAL_IP Alias of CF_INSTANCE_IP VCAP_APP_HOST Alias of CF_INSTANCE_IP CF_INSTANCE_PORT The cluster-visible port of the Build. LANG Required by Buildpacks to ensure consistent script load order. MEMORY_LIMIT The maximum amount of memory in MB the Build can consume. VCAP_APPLICATION A JSON structure containing App metadata. VCAP_SERVICES A JSON structure specifying bound services. Note: The environment variables Kf provides to Builds are a subset of those provided to App Runtime. ","categories":"","description":"Reference guide for the application build container environment.","excerpt":"Reference guide for the application build container environment.","ref":"/kf/docs/v2.11/developer/build-and-deploy/build-runtime/","tags":"","title":"Build runtime"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kf/categories/","tags":"","title":"Categories"},{"body":"The following steps guide you through configuring role-based access control (RBAC) in a Kf Space.\nBefore you begin Please follow the GKE RBAC guide before continuing with the following steps.\nConfigure Identity and Access Management (IAM) In addition to permissions granted through Kf RBAC, users, groups, or service accounts must also be authenticated to view GKE clusers at the project level. This requirement is the same as for configuring GKE RBAC, meaning users/groups must have at least the container.clusters.get IAM permission in the project containing the cluster. This permission is included by the container.clusterViewer role, and other more privilleged roles. For more information, review Interaction with Identity and Access Management.\nAssign container.clusterViewer to a user or group.\ngcloud projects add-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --role=\"container.clusterViewer\" \\ --member=\"${MEMBER}\" Example member values are:\nuser:test-user@gmail.com group:admins@example.com serviceAccount:test123@example.domain.com Manage Space membership as SpaceManager The cluster admin role, or members with SpaceManager role, can assign role to a user, group or service account.\nkf set-space-role MEMBER -t [Group|ServiceAccount|User] The cluster admin role, or members with SpaceManager role, can remove a member from a role.\nkf unset-space-role MEMBER -t [Group|ServiceAccount|User] You can view members and their roles within a Space.\nkf space-users Examples Assign SpaceDeveloper role to a user.\nkf set-space-role alice@example.com SpaceDeveloper Assign SpaceDeveloper role to a group.\nkf set-space-role devs@example.com SpaceDeveloper -t Group Assign SpaceDeveloper role to a Service Account.\nkf set-space-role sa-dev@example.domain.com SpaceDeveloper -t ServiceAccount App development as SpaceDeveloper Members with SpaceDeveloper role can perform Kf App development operations within the Space.\nTo push an App:\nkf push app_name -p [PATH_TO_APP_ROOT_DIRECTORY] To view logs of an App:\nkf logs app_name SSH into a Kubernetes Pod running the App:\nkf ssh app_name View available service brokers:\nkf marketplace View Apps as SpaceManager or SpaceAuditor Members with SpaceManager or SpaceAuditor role could view available Apps within the Space:\nkf apps View Kf Spaces within a cluster All roles (SpaceManager, SpaceDeveloper, and SpaceAuditor) can view available Kf Spaces within a cluster:\nkf spaces View Space members and their roles within a Space.\nkf space-users Impersonation flags To verify a member’s permission, a member with more priviliaged permission can test another member’s permissions using the impersonation flags: --as and --as-group.\nFor example, as a cluster admin, you can verify if a user (username: bob) has permission to push an App.\nkf push APP_NAME --as bob Verify a group (manager-group@example.com) has permission to assign permission to other members.\nkf set-space-role bob SpaceDeveloper --as-group manager-group@example.com ","categories":"","description":"Learn to grant users different roles on a cluster.","excerpt":"Learn to grant users different roles on a cluster.","ref":"/kf/docs/v2.11/operator/security/rbac/","tags":"","title":"Configure role-based access control"},{"body":"This page describes how routes and domains work in Kf, and how developers and administrators configure routes and domains for an App deployed on Kf cluster.\nYou must create domain and routes to give external access to your application.\nInternal routing Kf apps can communicate internally with other apps in the cluster directly using a service mesh without leaving the cluster network. By default, all traffic on the service mesh is encrypted using mutual TLS.\nAll apps deployed in the Kf cluster come with an internal endpoint configured by default. You can use the address app-name.space-name.svc.cluster.local for internal communication between apps. To use this internal address no extra steps are required. Mutual TLS is enabled by default for internal routes. Note that this internal address is only accessible from the pods running the apps and not accessible from outside the cluster.\nApp load balancing Traffic is routed by Istio to healthy instances of an App using a round-robin policy. Currently, this policy can’t be changed.\nRoute capabilities Routes tell the cluster’s ingress gateway where to deliver traffic and what to do if no Apps are available on the given address. By default, if no App is available on a Route and the Route receives a request it returns an HTTP 503 status code.\nRoutes are comprised of three parts: host, domain, and path. For example, in the URI payroll.mydatacenter.example.com/login:\nThe host is payroll The domain is mydatacenter.example.com The path is /login Routes must contain a domain, but the host and path is optional. Multiple Routes can share the same host and domain if they specify different paths. Multiple Apps can share the same Route and traffic will be split between them. This is useful if you need to support legacy blue/green deployments. If multiple Apps are bound to different paths, the priority is longest to shortest path.\nWarning: Kf doesn’t currently support TCP port-based routing. You must use a Kubernetes LoadBalancer if you want to expose a TCP port to the Internet. Ports are available on the cluster internal App address \u003capp-name\u003e.\u003cspace\u003e.\nManage routes The following sections describe how to use the kf CLI to manage Routes.\nList routes Developers can list Routes for the current Space using the kf routes command.\n$ kf routes Getting Routes in Space: my-space Found 2 Routes in Space my-space HOST DOMAIN PATH APPS echo example.com / echo * example.com /login uaa Create a route Developers can create Routes using the kf create-route command.\n# Create a Route in the targeted Space to match traffic for myapp.example.com/* $ kf create-route example.com --hostname myapp # Create a Route in the Space myspace to match traffic for myapp.example.com/* $ kf create-route -n myspace example.com --hostname myapp # Create a Route in the targeted Space to match traffic for myapp.example.com/mypath* $ kf create-route example.com --hostname myapp --path /mypath # You can also supply the Space name as the first parameter if you have # scripts that rely on the old cf style API. $ kf create-route myspace example.com --hostname myapp # myapp.example.com After a Route is created, if no Apps are bound to it then an HTTP 503 status code is returned for any matching requests.\nNote: Routes that share the same host and domain must be in the same Space. Map a route to your app Developers can make their App accessible on a Route using the kf map-route command.\n$ kf map-route MYAPP mycluster.example.com --host myapp --path mypath Note: map-route creates the Route if it doesn’t exist yet. Unmap a route Developers can remove their App from being accessible on a Route using the kf unmap-route command.\n$ kf unmap-route MYAPP mycluster.example.com --host myapp --path mypath Delete a route Developers can delete a Route using the kf delete-route command.\n$ kf delete-route mycluster.example.com --host myapp --path mypath Deleting a Route will stop traffic from being routed to all Apps listening on the Route.\nManage routes declaratively in your app manifest Routes can be managed declaratively in your app manifest file. They will be created if they do not yet exist.\n--- applications: - name: my-app # ... routes: - route: example.com - route: www.example.com/path You can read more about the supported route properties in the manifest documentation.\nNote: Declaring Routes in your manifest file only creates new Routes, it does not delete Routes you created manually or as part of a previous push. Routing CRDs There are four types that are relevant to routing:\nVirtualService Route Service App Each App has a Service, which is an abstract name given to all running instances of your App. The name of the Service is the same as the App. A Route represents a single external URL. Routes constantly watch for changes to Apps, when an App requests to be added to a Route, the Route updates its list of Apps and then the VirtualService. A VirtualService represents a single domain and merges a list of all Routes in a Space that belong to that domain.\nIstio reads the configuration on VirtualServices to determine how to route traffic.\n","categories":"","description":"Make your app reachable over the network.","excerpt":"Make your app reachable over the network.","ref":"/kf/docs/v2.11/developer/routes-and-domains/","tags":"","title":"Configure routes and domains"},{"body":"You can use Google Cloud Monitoring dashboards to create custom dashboards and charts. Kf comes with a default template which can be used to create dashboards to monitor the performance of your applications.\nApplication performance dashboard Run the following commands to deploy a dashboard in your monitoring workspace in Cloud monitoring dashboards to monitor performance of your apps. This dashboard has application performance metrics like requests/sec, round trip latency, HTTP error codes, and more.\ngit clone https://github.com/google/kf cd ./kf/dashboards ./create-dashboard.py my-dashboard my-cluster my-space System resources and performance dashboard You can view all the system resources and performance metrics such as list of nodes, pods, containers and much more using a built-in dashboard. Click the link below to access the system dashboard.\nSystem dashboard\nMore details about this dashboard can be found here.\nCreate SLO and alerts You can create SLOs and Alerts on available metrics to monitor performance and availability of both system and applications. For example, you can use the metrics istio.io/service/server/response_latencies to setup an alert on the application roundtrip latency.\nConfigure dashboard access control Follow these instructions to provide dashboard access to developers and other members on the team. The role roles/monitoring.dashboardViewer provides read-only access to dashboards.\n","categories":"","description":"","excerpt":"You can use Google Cloud Monitoring dashboards to create custom …","ref":"/kf/docs/v2.11/operator/logging-monitoring/monitoring-dashboard/","tags":"","title":"Create and user monitoring dashboards."},{"body":"The Stack configuration can be updated by editing the kfsystem Custom Resource:\nkubectl edit kfsystem kfsystem This example sets the Google Cloud buildpacks a V3 Stack:\nspec: kf: config: spaceStacksV3: - name: google description: Google buildpacks (https://github.com/GoogleCloudPlatform/buildpacks) buildImage: gcr.io/buildpacks/builder:v1 runImage: gcr.io/buildpacks/gcp/run:v1 This new Stack can now be pushed:\nkf push myapp --stack google This example configures the Ruby V2 buildpack and sets the build pipeline defaults to use V2 Stacks:\nspec: kf: config: spaceDefaultToV3Stack: false spaceBuildpacksV2: - name: ruby_buildpack url: https://github.com/cloudfoundry/ruby-buildpack spaceStacksV2: - name: cflinuxfs3 image: cloudfoundry/cflinuxfs3@sha256:5219e9e30000e43e5da17906581127b38fa6417f297f522e332a801e737928f5 ","categories":"","description":"","excerpt":"The Stack configuration can be updated by editing the kfsystem Custom …","ref":"/kf/docs/v2.11/operator/buildpacks/configure-stacks/","tags":"","title":"Customize stacks"},{"body":"Buildpacks are used by Kf to turn an application’s source code into an executable image. Cloud Native buildpacks use the latest Buildpack API v3. Companies are actively adding v3 support to existing buildpacks.\nKf supports buildpacks that conform to both V2 and V3 of the Buildpack API specification.\nCompare V2 and V3 buildpacks V2 buildpacks V3 buildpacks Alternate names Cloud Foundry buildpacks Cloud Native buildpacks (CNB), Builder Images Status Being replaced Current Ownership Cloud Foundry Buildpacks.io Stack Shared by builder and runtime Optionally different for builder and runtime Local development Not possible Yes, with the pack CLI Custom buildpacks Available at runtime Must be built into the builder Buildpack lifecycle Step Cloud Foundry Kf with buildpacks V2 Kf with buildpacks V3 Source location BITS service Container registry Container registry Buildpack location BOSH/HTTP HTTP Container registry Stack location BOSH Container registry Container registry Result Droplet (App binary without stack) Image (Droplet on a stack) Image Runtime Droplet glued on top of stack and run Run produced image Run produced image Kf always produces a full, executable image as a result of its build process. Cloud Foundry, on the other hand, produces parts of an executable image at build time and the rest is added at runtime.\nKf chose to follow the model of always producing a full image for the following reasons:\nImages can be exported, run locally, and inspected statically Better security and auditing with tools like binary authorization App deployments are reproducible Kf and buildpacks Kf stores its global list of buildpacks and stacks in the config-defaults ConfigMap in the kf Namespace. Modification of the buildpacks and stacks properties should be done at the kfsystem Custom Resource, the Kf operator automatically updates the config-defaults ConfigMap based on the values set at kfsystem.\nEach Space reflects these buildpacks in its status field. For a Space named buildpack-docs you could run the following to see the full Space configuration:\nkf space buildpack-docs Getting Space buildpack-docs API Version: kf.dev/v1alpha1 Kind: Space Metadata: Creation Timestamp: 2020-02-14T15:09:52Z Name: buildpack-docs Self Link: /apis/kf.dev/v1alpha1/spaces/buildpack-docs UID: 0cf1e196-4f3c-11ea-91a4-42010a80008d Status: Build Config: Buildpacks V2: - Name: staticfile_buildpack URL: https://github.com/cloudfoundry/staticfile-buildpack Disabled: false - Name: java_buildpack URL: https://github.com/cloudfoundry/java-buildpack Disabled: false Stacks V2: - Image: cloudfoundry/cflinuxfs3 Name: cflinuxfs3 Stacks V3: - Build Image: cloudfoundry/cnb:cflinuxfs3 Description: A large Cloud Foundry stack based on Ubuntu 18.04 Name: org.cloudfoundry.stacks.cflinuxfs3 Run Image: cloudfoundry/run:full-cnb Under the Build Config section there are three fields to look at:\nBuildpacks V2 contains a list of V2 compatible buildpacks in the order they’ll be run Stacks V2 indicates the stacks that can be chosen to trigger a V2 buildpack build Stacks V3 indicates the stacks that can be chosen to trigger a V3 buildpack build You can also list the stacks with kf stacks:\nkf stacks Getting stacks in Space: buildpack-docs Version Name Build Image Run Image Description V2 cflinuxfs3 cloudfoundry/cflinuxfs3 cloudfoundry/cflinuxfs3 V3 org.cloudfoundry.stacks.cflinuxfs3 cloudfoundry/cnb:cflinuxfs3 cloudfoundry/run:full-cnb A large Cloud Foundry stack based on Ubuntu 18.04 Because V3 build images already have their buildpacks built-in, you must use kf buildpacks to get the list:\nkf buildpacks Getting buildpacks in Space: buildpack-docs Buildpacks for V2 stacks: Name Position URL staticfile_buildpack 0 https://github.com/cloudfoundry/staticfile-buildpack java_buildpack 1 https://github.com/cloudfoundry/java-buildpack V3 Stack: org.cloudfoundry.stacks.cflinuxfs3: Name Position Version Latest org.cloudfoundry.jdbc 0 v1.0.179 true org.cloudfoundry.jmx 1 v1.0.180 true org.cloudfoundry.go 2 v0.0.2 true org.cloudfoundry.tomcat 3 v1.1.102 true org.cloudfoundry.distzip 4 v1.0.171 true org.cloudfoundry.springboot 5 v1.1.2 true ... Customize V3 buildpacks You can customize the buildpacks that are available to your developers by creating your own builder image with exactly the buildpacks they should have access to. You can also use builder images published by other authors.\nUse a third-party builder image A list of published CNB stacks is available from the Buildpack CLI pack. As of this writing, pack suggest-stacks outputs:\npack suggest-stacks Stacks maintained by the community: Stack ID: heroku-18 Description: The official Heroku stack based on Ubuntu 18.04 Maintainer: Heroku Build Image: heroku/pack:18-build Run Image: heroku/pack:18 Stack ID: io.buildpacks.stacks.bionic Description: A minimal Cloud Foundry stack based on Ubuntu 18.04 Maintainer: Cloud Foundry Build Image: cloudfoundry/build:base-cnb Run Image: cloudfoundry/run:base-cnb Stack ID: org.cloudfoundry.stacks.cflinuxfs3 Description: A large Cloud Foundry stack based on Ubuntu 18.04 Maintainer: Cloud Foundry Build Image: cloudfoundry/build:full-cnb Run Image: cloudfoundry/run:full-cnb Stack ID: org.cloudfoundry.stacks.tiny Description: A tiny Cloud Foundry stack based on Ubuntu 18.04, similar to distroless Maintainer: Cloud Foundry Build Image: cloudfoundry/build:tiny-cnb Run Image: cloudfoundry/run:tiny-cnb To modify Kf to use the stack published by Heroku, edit kfsystem Custom Resource, which automatically updates the config-defaults ConfigMap in the kf Namespace. Add an entry to the spaceStacksV3 key like the following:\nkubectl edit kfsystem kfsystem spaceStacksV3: | - name: org.cloudfoundry.stacks.cflinuxfs3 description: A large Cloud Foundry stack based on Ubuntu 18.04 buildImage: cloudfoundry/cnb:cflinuxfs3 runImage: cloudfoundry/run:full-cnb - name: heroku-18 description: The official Heroku stack based on Ubuntu 18.04 buildImage: heroku/pack:18-build runImage: heroku/pack:18 Then, run stacks again:\nkf stacks Getting stacks in Space: buildpack-docs Version Name Build Image Run Image Description V2 cflinuxfs3 cloudfoundry/cflinuxfs3 cloudfoundry/cflinuxfs3 V3 org.cloudfoundry.stacks.cflinuxfs3 cloudfoundry/cnb:cflinuxfs3 cloudfoundry/run:full-cnb A large Cloud Foundry stack based on Ubuntu 18.04 V3 heroku-18 heroku/pack:18-build heroku/pack:18 The official Heroku stack based on Ubuntu 18.04 Create your own builder image The Buildpack CLI pack is used to create your own builder image. You can follow pack’s Working with builders using create-builder documentation to create your own builder image. After it’s created, push it to a container registry and add it to the kfsystem Custom Resource.\nSet a default stack Apps will be assigned a default stack if one isn’t supplied in their manifest. The default stack is the first in the V2 or V3 stacks list. Unless overridden, a V2 stack is chosen for compatibility with Cloud Foundry.\nYou can force Kf to use a V3 stack instead of a V2 by setting the spaceDefaultToV3Stack field in the kfsystem Custom Resource to be \"true\" (kfsystem automatically updates corresonding spaceDefaultToV3Stack field in the config-defaults ConfigMap):\nkubectl edit kfsystem kfsystem spaceDefaultToV3Stack: \"true\" This option can also be modified on a per-Space basis by changing setting the spec.buildConfig.defaultToV3Stack field to be true or false. If unset, the value from the config-defaults ConfigMap is used.\nconfig-defaults value for spaceDefaultToV3Stack Space’s spec.buildConfig.defaultToV3Stack Default stack unset unset V2 \"false\" unset V2 \"true\" unset V3 any false V2 any true V3 ","categories":"","description":"","excerpt":"Buildpacks are used by Kf to turn an application’s source code into an …","ref":"/kf/docs/v2.11/operator/buildpacks/customizing-buildpacks/","tags":"","title":"Customize stacks and buildpacks"},{"body":"This page shows you how to deploy Kf Cloud Service Broker and use it to provision or deprovision backing resources. Read about the concepts and architecture to learn more about the Kf Cloud Service Broker.\nCreate environment variables Linux\nexport PROJECT_ID=YOUR_PROJECT_ID export CLUSTER_PROJECT_ID=YOUR_PROJECT_ID export CLUSTER_NAME=kf-cluster export INSTANCE_NAME=cloud-service-broker export COMPUTE_REGION=us-central1 Windows PowerShell\nSet-Variable -Name PROJECT_ID -Value YOUR_PROJECT_ID Set-Variable -Name CLUSTER_PROJECT_ID -Value YOUR_PROJECT_ID Set-Variable -Name CLUSTER_NAME -Value kf-cluster Set-Variable -Name INSTANCE_NAME -Value cloud-service-broker Set-Variable -Name COMPUTE_REGION -Value us-central1 Set up the Kf Cloud Service Broker database Create a MySQL instance.\nNote: Read [Creating and managing MySQL users](https://cloud.google.com/sql/docs/mysql/create-manage-users) for Google Cloud SQL and set a secure password for the default `root` user. gcloud sql instances create ${INSTANCE_NAME} --cpu=2 --memory=7680MB --require-ssl --region=${COMPUTE_REGION} Create a database named servicebroker in the MySQL instance.\nNote: Document the database name since it is used in later steps. gcloud sql databases create servicebroker -i ${INSTANCE_NAME} Create a username and password to be used by the broker.\nNote: Document these values since they will be used in later steps. gcloud sql users create csbuser -i ${INSTANCE_NAME} --password=csbpassword Set up a Google Service Account for the broker Create a Google Service Account. gcloud iam service-accounts create csb-${CLUSTER_NAME}-sa \\ --project=${CLUSTER_PROJECT_ID} \\ --description=\"GSA for CSB at ${CLUSTER_NAME}\" \\ --display-name=\"csb-${CLUSTER_NAME}\" Grant roles/cloudsql.client permissions to the Service Account. This is required to connect the service broker pod to the CloudSQL for MySQL instance through the CloudSQL Proxy. gcloud projects add-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --member=\"serviceAccount:csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com\" \\ --role=\"roles/cloudsql.client\" Grant additional Google Cloud permissions to the Service Account. Note: In the example below, we grant IAM roles required to provision an instance of CloudSQL and Cloud Memorystore. You must grant this service account the appropriate roles to provision instances of other Google Cloud managed services listed in kf marketplace. gcloud projects add-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --member=\"serviceAccount:csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com\" \\ --role=\"roles/compute.networkUser\" gcloud projects add-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --member=\"serviceAccount:csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com\" \\ --role=\"roles/cloudsql.admin\" gcloud projects add-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --member=\"serviceAccount:csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com\" \\ --role=\"roles/redis.admin\" Verify the permissions. Warning: Replace the CSB_SERVICE_ACCOUNT_NAME variable in the YAML below with the full service account resolved from csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com. gcloud projects get-iam-policy ${CLUSTER_PROJECT_ID} \\ --filter='bindings.members:serviceAccount:\"CSB_SERVICE_ACCOUNT_NAME\"' \\ --flatten=\"bindings[].members\" Set up Workload Identity for the broker Bind the Google Service Account with the Kubernetes Service Account. gcloud iam service-accounts add-iam-policy-binding \"csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com\" \\ --project=${CLUSTER_PROJECT_ID} \\ --role=\"roles/iam.workloadIdentityUser\" \\ --member=\"serviceAccount:${CLUSTER_PROJECT_ID}.svc.id.goog[kf-csb/csb-user]\" Verify the binding. gcloud iam service-accounts get-iam-policy \"csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com\" \\ --project=${CLUSTER_PROJECT_ID} Set up a Kubernetes Secret to share configuration with the broker Create a config.yml file. Note: Replace the default user/password if desired. Ensure you have set the CLUSTER_PROJECT_ID in the Create environment variables step.\ncat \u003c\u003c EOF \u003e\u003e ./config.yml gcp: credentials: \"\" project: ${CLUSTER_PROJECT_ID} db: host: 127.0.0.1 password: csbpassword user: csbuser tls: false api: user: servicebroker password: password EOF Create the kf-csb namespace. kubectl create ns kf-csb Create the Kubernetes Secret. kubectl create secret generic csb-secret --from-file=config.yml -n kf-csb Install the Kf Cloud Service Broker Download the kf-csb.yml. gsutil cp gs://kf-releases/csb/v{{this_kf_csb_version}}/kf-csb.yaml /tmp/kf-csb.yaml Edit /tmp/kf-csb.yaml and replace placeholders with final values. In the example below, sed is used. sed -i \"s|\u003cGSA_NAME\u003e|csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com|g\" /tmp/kf-csb.yaml sed -i \"s|\u003cINSTANCE_CONNECTION_NAME\u003e|${CLUSTER_PROJECT_ID}:${COMPUTE_REGION}:${INSTANCE_NAME}|g\" /tmp/kf-csb.yaml sed -i \"s|\u003cDB_PORT\u003e|3306|g\" /tmp/kf-csb.yaml Apply yaml for Kf Cloud Service Broker. kubectl apply -f /tmp/kf-csb.yaml Verify the Cloud Service Broker installation status. kubectl get pods -n kf-csb Create a service broker Note: The user/password must match what you entered in the Kubernetes secret step earlier. kf create-service-broker cloud-service-broker servicebroker password http://csb-controller.kf-csb/ Validate installation Check for available services in the marketplace.\nkf marketplace If everything is installed and configured correctly, you should see the following:\n$ kf marketplace Broker Name Namespace Description cloud-service-broker csb-google-bigquery A fast, economical and fully managed data warehouse for large-scale data analytics. cloud-service-broker csb-google-dataproc Dataproc is a fully-managed service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. cloud-service-broker csb-google-mysql Mysql is a fully managed service for the Google Cloud Platform. cloud-service-broker csb-google-postgres PostgreSQL is a fully managed service for the Google Cloud Platform. cloud-service-broker csb-google-redis Cloud Memorystore for Redis is a fully managed Redis service for the Google Cloud Platform. cloud-service-broker csb-google-spanner Fully managed, scalable, relational database service for regional and global application data. cloud-service-broker csb-google-stackdriver-trace Distributed tracing service cloud-service-broker csb-google-storage-bucket Google Cloud Storage that uses the Terraform back-end and grants service accounts IAM permissions directly on the bucket. Clean up Delete cloud-service-broker. kf delete-service-broker cloud-service-broker Delete CSB components. kubectl delete ns kf-csb Delete the broker’s database instance.\ngcloud sql instances delete ${INSTANCE_NAME} --project=${CLUSTER_PROJECT_ID} Remove the IAM policy bindings.\ngcloud projects remove-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --member='serviceAccount:csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com' \\ --role=roles/cloudsql.client gcloud projects remove-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --member='serviceAccount:csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com' \\ --role=roles/compute.networkUser gcloud projects remove-iam-policy-binding ${CLUSTER_PROJECT_ID} \\ --member='serviceAccount:csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com' \\ --role=roles/redis.admin Remove the GSA.\ngcloud iam service-accounts delete csb-${CLUSTER_NAME}-sa@${CLUSTER_PROJECT_ID}.iam.gserviceaccount.com \\ --project=${CLUSTER_PROJECT_ID} ","categories":"","description":"","excerpt":"This page shows you how to deploy Kf Cloud Service Broker and use it …","ref":"/kf/docs/v2.11/operator/service-brokers/deploying-cloud-sb/","tags":"","title":"Deploy Kf Cloud Service Broker"},{"body":"This document shows how to deploy Spring Cloud Config in a Kf cluster.\nSpring Cloud Config provides a way to decouple application code from its runtime configuration. The Spring Cloud Config configuration server can read configuration files from Git repositories, the local filesystem, HashiCorp Vault servers, or Cloud Foundry CredHub. Once the configuration server has read the configuration, it can format and serve that configuration as YAML, Java Properties, or JSON over HTTP.\nBefore you begin You will need a cluster with Kf installed and access to the Kf CLI.\nAdditionally, you will need the following software:\ngit: Git is required to clone a repository. Download the Spring Cloud Config configuration server To download the configuration server source:\nOpen a terminal.\nClone the source for the configuration server:\ngit clone --depth 1 \"https://github.com/google/kf\" Configure and deploy a configuration server To update the settings for the instance:\nChange directory to spring-cloud-config-server:\ncd kf/spring-cloud-config-server Open manifest.yaml.\nChange the GIT_URI environment variable to the URI of your Git configuration server.\nOptionally, change the name of the application in the manifest.\nOptionally, configure additional properties or alternative property sources by editing src/main/resources/application.properties.\nDeploy the configuration server without an external route. If you changed the name of the application in the manifest, update it here:\nkf push --no-route spring-cloud-config Note: The default configuration is not production ready. The README.md file contains additional steps to take if you want to deploy the application to production. Bind applications to the configuration server You can create a user provided service to bind the deployed configuration server to other Kf applications in the same cluster or namespace.\nHow you configure them will depend on the library you use:\nApplications using Pivotal’s Spring Cloud services client library Existing PCF applications that use Pivotal’s Spring Cloud Services client library can be bound using the following method:\nCreate a user provided service named config-server. This step only has to be done once per configuration server:\nkf cups config-server -p '{\"uri\":\"http://spring-cloud-config\"}' -t configuration Note: If you want to use a configuration server in a different space, change the URI to include the space. For each application that needs get credentials, run:\nkf bind-service application-name config-server kf restart application-name This will create an entry into the VCAP_SERVICES environment variable for the configuration server.\nOther applications Applications that can connect directly to a Spring Cloud Config configuration server should be configured to access it using its cluster internal URI:\nhttp://spring-cloud-config Note: If you want to use a configuration server in a different space, change the URI to include the space name. For Spring applications that use the Spring Cloud Config client library can set the spring.cloud.config.uri property in the appropriate location for your application. This is usually an application.properties or application.yaml file. For other frameworks, see your library’s reference information. Delete the configuration server To remove a configuration server:\nRemove all bindings to the configuration server running the following commands for each bound application:\nkf unbind-service application-name config-server kf restart application-name Remove the service entry for the configuration server:\nkf delete-service config-server Delete the configuration server application:\nkf delete spring-cloud-config What’s next Read more about the types of configuration sources Spring Cloud Config supports. Learn about the structure of the VCAP_SERVICES environment variable to understand how it can be used for service discovery. ","categories":"","description":"Learn how to deploy Spring Cloud Config as a configuration server.\n","excerpt":"Learn how to deploy Spring Cloud Config as a configuration server.\n","ref":"/kf/docs/v2.11/examples/deploying-spring-cloud-config/","tags":"","title":"Deploy Spring Cloud Config"},{"body":"These instructions will walk you through deploying the Cloud Foundry Spring Music reference App using the Kf Cloud Service Broker.\nBuilding Java Apps from source: The Spring Music source will be built on the cluster, not locally.\nService broker integration: You will create a database using the Kf Cloud Service Broker and bind the Spring Music App to it.\nSpring Cloud Connectors: Spring Cloud Connectors are used by the Spring Music App to detect things like bound CF services. They work seamlessly with Kf.\nConfiguring the Java version: You will specify the version of Java you want the buildpack to use.\nPrerequisites Install and configure the Kf Cloud Service Broker.\nDeploy Spring Music Clone source Clone the Spring Music repo.\ngit clone https://github.com/cloudfoundry-samples/spring-music.git spring-music cd spring-music Edit manifest.yml, and replace path: build/libs/spring-music-1.0.jar with stack: org.cloudfoundry.stacks.cflinuxfs3. This instructs Kf to build from source using cloud native buildpacks so you don’t have to compile locally.\n--- applications: - name: spring-music memory: 1G random-route: true stack: org.cloudfoundry.stacks.cflinuxfs3 env: JBP_CONFIG_SPRING_AUTO_RECONFIGURATION: '{enabled: false}' # JBP_CONFIG_OPEN_JDK_JRE: '{ jre: { version: 11.+ } }' Push Spring Music with no bindings Create and target a Space.\nkf create-space test kf target -s test Deploy Spring Music.\nkf push spring-music Use the proxy feature to access the deployed App.\nStart the proxy:\nkf proxy spring-music Open http://localhost:8080 in your browser:\nThe deployed App includes a UI element showing which (if any) Spring profile is being used. No profile is being used here, indicating an in-memory database is in use.\nCreate and bind a database Create a PostgresSQL database from the marketplace.\nNote: You must set the COMPUTE_REGION and VPC_NAME variables so Kf Cloud Service Broker knows where to provision your instance, and authorize the VPC Kf Apps run on to access it. kf create-service csb-google-postgres small spring-music-postgres-db -c '{\"region\":\"COMPUTE_REGION\",\"authorized_network\":\"VPC_NAME\"}' Bind the Service with the App.\nkf bind-service spring-music spring-music-postgres-db Restart the App to make the service binding available via the VCAP_SERVICES environment variable.\nkf restart spring-music (Optional) View the binding details.\nkf bindings Verify the App is using the new binding.\nStart the proxy:\nkf proxy spring-music Open http://localhost:8080 in your browser:\nYou now see the Postgres profile is being used, and we see the name of our Service we bound the App to.\nClean up Unbind and delete the PostgreSQL service:\nkf unbind-service spring-music spring-music-db kf delete-service spring-music-db Delete the App:\nkf delete spring-music ","categories":"","description":"Learn to deploy an app with backing services.\n","excerpt":"Learn to deploy an app with backing services.\n","ref":"/kf/docs/v2.11/examples/spring-music/","tags":"","title":"Deploy Spring Music"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kf/docs/","tags":"","title":"Docs"},{"body":"Kf Apps can be deployed on dedicated nodes in the cluster. This feature is required if you have the circumstances where you might want more control on a node where an App Pod lands. For example:\nIf you are sharing the same cluster for different Apps but want dedicated nodes for a particular App. If you want dedicated nodes for a given organization (Kf Space). If you want to target a specific operating system like Windows. If you want to co-locate Pods from two different services that frequently communicate. To enable compute isolation, Kf uses the Kubernetes nodeSelector. To use this feature, first add labels on the nodes or node pools where you want your App Pods to land and then add the same qualifying labels on the Kf Space. All the Apps installed in this Space then land on the nodes with matching labels.\nKf creates a Kubernetes pod to execute each Kf Build, the buildNodeSelector feature can be used to isolate compute resources to execute only the Build pods. One use case is to isolate Build pods to run on nodes with SSD, while running the App pods on other nodes. The BuildNodeSelectors feature provides compute resource optimization and flexibility in the cluster. Please refer to chapter ‘Configure BuildNodeSelectors and a build node pool’ on this page.\nConfigure nodeSelector in a Kf cluster By default, compute isolation is disabled. Use the following procedure to configure labels and nodeSelector.\nAdd a label (distype=ssd) on the node where you want your application pods to land.\nkubectl label nodes nodeid disktype=ssd Add the same label on the Kf Space. All Apps deployed in this Space will then land on the qualifying nodes.\nkf configure-space set-nodeselector space-name disktype ssd You can add multiple labels by running the same command again.\nCheck the label is configured.\nkf configure-space get-nodeselector space-name Delete the label from the space.\nkf configure-space unset-nodeselector space-name disktype Override nodeSelector for Kf stacks Deployment of Kf Apps can be further targeted based on what stack is being used to build and package the App. For example, if you want your applications built with spaceStacksV2 to land on nodes with Linux kernel 4.4.1., nodeSelector values on a stack override the values configured on the Space.\nTo configure the nodeSelector on a stack:\nEdit the config-defaults of your Kf cluster and add the labels.\n$ kubectl -n kf edit configmaps config-defaults Add nodeSelector to the stacks definition.\n..... ..... spaceStacksV2: | - name: cflinuxfs3 image: cloudfoundry/cflinuxfs3 nodeSelector: OS_KERNEL: LINUX_4.4.1 ..... ..... Configure BuildNodeSelectors and a Build node pool Build node selectors are only effective at overriding the node selectors for the Build pods, they do not affect App pods. For example, if you specify both the node selectors on the Space and the Build node selectors in Kfsystem, App pods will have the Space node selectors while the Build pods will have the Build node selectors from Kfsystem; if node selectors are only specified in the Space, both the App and Build pods will have the node selector from the Space.\nAdd labels (disktype:ssd for example) on the nodes that you want your Build pods to be assigned to.\nkubectl label nodes nodeid disktype=ssd Add/update Build node selectors (in the format of key:value pairs) by patching KfSystem CR.\nkubectl patch kfsystem kfsystem --type='json' -p='[{'op': 'replace', 'path': '/spec/kf/config/buildNodeSelectors', 'value': {\u003ckey\u003e:\u003cvalue\u003e}}]' For example, to add disktype=ssd as the Build node selector:\nkubectl patch kfsystem kfsystem --type='json' -p='[{'op': 'replace', 'path': '/spec/kf/config/buildNodeSelectors', 'value': {\"disktype\":\"ssd\"}}]' ","categories":"","description":"Isolate the underlying nodes certain apps or builds are scheuled onto.","excerpt":"Isolate the underlying nodes certain apps or builds are scheuled onto.","ref":"/kf/docs/v2.11/operator/security/compute-isolation/","tags":"","title":"Enable compute isolation"},{"body":"Kf requires Kubernetes and several other OSS projects to run. Some of the dependencies are satisfied with Google-managed services—for example, GKE provides Kubernetes.\nDependencies GKE Anthos Service Mesh Tekton Pipelines Get CRD details Kf supports the kubectl subcommand explain. It allows you to list the fields in Kf CRDs to understand how to create Kf objects via automation instead of manually via the CLI. This command is designed to be used with ConfigSync to automate creation and management of resources like Spaces across many clusters. You can use this against any of the component kinds below.\nIn this example, we examine the kind called space in the spaces CRD:\nkubectl explain space.spec The output looks similar to this:\n$ kubectl explain space.spec KIND: Space VERSION: kf.dev/v1alpha1 RESOURCE: spec \u003cObject\u003e DESCRIPTION: SpaceSpec contains the specification for a space. FIELDS: buildConfig \u003cObject\u003e BuildConfig contains config for the build pipelines. networkConfig \u003cObject\u003e NetworkConfig contains settings for the space's networking environment. runtimeConfig \u003cObject\u003e RuntimeConfig contains settings for the app runtime environment. Kf components Kf installs several of its own Kubernetes custom resources and controllers. The custom resources effectively serve as the Kf API and are used by the kf CLI to interact with the system. The controllers use Kf’s CRDs to orchestrate the other components in the system.\nYou can view the CRDs installed and used by Kf by running the following command:\nkubectl api-resources --api-group=kf.dev The output of that command is as follows:\nNAME SHORTNAMES APIGROUP NAMESPACED KIND apps kf.dev true App builds kf.dev true Build clusterservicebrokers kf.dev false ClusterServiceBroker routes kf.dev true Route servicebrokers kf.dev true ServiceBroker serviceinstancebindings kf.dev true ServiceInstanceBinding serviceinstances kf.dev true ServiceInstance spaces kf.dev false Space Apps Apps represent a twelve-factor application deployed to Kubernetes. They encompass source code, configuration, and the current state of the application. Apps are responsible for reconciling:\nKf Builds Kf Routes Kubernetes Deployments Kubernetes Services Kubernetes ServiceAccounts Kubernetes Secrets You can list Apps using Kf or kubectl:\nkf apps kubectl get apps -n space-name Builds Builds combine the source code and build configuration for Apps. They provision Tekton TaskRuns with the correct steps to actuate a Buildpack V2, Buildpack V3, or Dockerfile build.\nYou can list Builds using Kf or kubectl:\nkf builds kubectl get builds -n space-name ClusterServiceBrokers ClusterServiceBrokers hold the connection information necessary to extend Kf with a service broker. They are responsible for fetching the catalog of services the broker provides and displaying them in the output of kf marketplace.\nYou can list ClusterServiceBrokers using kubectl:\nkubectl get clusterservicebrokers Routes Routes are a high level structure that contain HTTP routing rules. They are responsible for reconciling Istio VirtualServices.\nYou can list Routes using Kf or kubectl:\nkf routes kubectl get routes -n space-name ServiceBrokers ServiceBrokers hold the connection information necessary to extend Kf with a service broker. They are responsible for fetching the catalog of services the broker provides and displaying them in the output of kf marketplace.\nYou can list ServiceBrokers using kubectl:\nkubectl get servicebrokers -n space-name ServiceInstanceBinding ServiceInstanceBindings hold the parameters to create a binding on a service broker and the credentials the broker returns for the binding. They are responsible for calling the bind API on the broker to bind the service.\nYou can list ServiceInstanceBindings using Kf or kubectl:\nkf bindings kubectl get serviceinstancebindings -n space-name ServiceInstance ServiceInstances hold the parameters to create a service on a service broker. They are responsible for calling the provision API on the broker to create the service.\nYou can list ServiceInstances using Kf or kubectl:\nkf services kubectl get serviceinstances -n space-name Spaces Spaces hold configuration information similar to Cloud Foundry organizations and spaces. They are responsible for:\nCreating the Kubernetes Namespace that other Kf resources are provisioned into. Creating Kubernetes NetworkPolicies to enforce network connection policies. Holding configuration and policy for Builds, Apps, and Routes. You can list Spaces using Kf or kubectl:\nkf spaces kubectl get spaces Kf RBAC / Permissions The following sections list permissions for Kf and its components to have correct access at the cluster level. These permissions are required and enabled by default in Kf; do not attempt to disable them.\nComponents Namespace Service Account controller kf controller subresource-apiserver kf controller webhook kf controller appdevexperience-operator appdevexperience appdevexperience-operator Note that the appdevexperience-operator service account has the same set of permissions as controller. The operator is what deploys all Kf components, including custom resource definitions and controllers.\nRBAC for Kf service accounts The following apiGroup definitions detail which access control permissions components in {{product_name}} have on which API groups and resources for both the controller and appdevexperience-operator service accounts.\n- apiGroups: - \"authentication.k8s.io\" resources: - tokenreviews verbs: - create - apiGroups: - \"authorization.k8s.io\" resources: - subjectaccessreviews verbs: - create - apiGroups: - \"\" resources: - pods - services - persistentvolumeclaims - persistentvolumes - endpoints - events - configmaps - secrets verbs: * - apiGroups: - \"\" resources: - services - services/status verbs: - create - delete - get - list - watch - apiGroups: - \"apps\" resources: - deployments - daemonsets - replicasets - statefulsets verbs: * - apiGroups: - \"apps\" resources: - deployments/finalizers verbs: - get - list - create - update - delete - patch - watch - apiGroups: - \"rbac.authorization.k8s.io\" resources: - clusterroles - roles - clusterrolebindings - rolebindings verbs: - create - delete - update - patch - escalate - get - list - deletecollection - bind - apiGroups: - \"apiregistration.k8s.io\" resources: - apiservices verbs: - update - patch - create - delete - get - list - apiGroups: - \"pubsub.cloud.google.com\" resources: - topics - topics/status verbs: * - apiGroups: - \"\" resources: - namespaces - namespaces/finalizers - serviceaccounts verbs: - get - list - create - update - watch - delete - patch - watch - apiGroups: - \"autoscaling\" resources: - horizontalpodautoscalers verbs: - create - delete - get - list - update - patch - watch - apiGroups: - \"coordination.k8s.io\" resources: - leases verbs: * - apiGroups: - \"batch\" resources: - jobs - cronjobs verbs: - get - list - create - update - patch - delete - deletecollection - watch - apiGroups: - \"messaging.cloud.google.com\" resources: - channels verbs: - delete - apiGroups: - \"pubsub.cloud.google.com\" resources: - pullsubscriptions verbs: - delete - get - list - watch - create - update - patch - apiGroups: - \"pubsub.cloud.google.com\" resources: - [pullsubscriptions/status verbs: - get - update - patch - apiGroups: - \"events.cloud.google.com\" resources: * verbs: * - apiGroups: - \"keda.k8s.io\" resources: * verbs: * - apiGroups: - \"admissionregistration.k8s.io\" resources: - mutatingwebhookconfigurations - validatingwebhookconfigurations verbs: - get - list - create - update - patch - delete - watch - apiGroups: - \"extensions\" resources: - ingresses - ingresses/status verbs: * - apiGroups: - \"\" resources: - endpoints/restricted verbs: - create - apiGroups: - \"certificates.k8s.io\" resources: - certificatesigningrequests - certificatesigningrequests/approval - certificatesigningrequests/status verbs: - update - create - get - delete - apiGroups: - \"apiextensions.k8s.io\" resources: - customresourcedefinitions verbs: - get - list - create - update - patch - delete - watch - apiGroups: - \"networking.k8s.io\" resources: - networkpolicies verbs: - get - list - create - update - patch - delete - deletecollection - watch - apiGroups: - \"\" resources: - nodes verbs: - get - list - watch - update - patch - apiGroups: - \"\" resources: - nodes/status verbs: - patch The following table lists how the RBAC permissions are used in Kf, where:\nview includes the verbs: get, list, watch modify includes the verbs: create, update, delete, patch Permissions Reasons Can view all secrets Kf reconcilers need to read secrets for functionalities such as space creation and service instance binding. Can modify pods Kf reconcilers need to modify pods for functionalities such as building/pushing Apps and Tasks. Can modify secrets Kf reconcilers need to modify secrets for functionalities such as building/pushing Apps and Tasks and service instance binding. Can modify configmaps Kf reconcilers need to modify configmaps for functionalities such as building/pushing Apps and Tasks. Can modify endpoints Kf reconcilers need to modify endpoints for functionalities such as building/pushing Apps and route binding. Can modify services Kf reconcilers need to modify pods for functionalities such as building/pushing Apps and route binding. Can modify events Kf controller creates and emits events for the resources managed by Kf. Can modify serviceaccounts Kf needs to modify service accounts for App deployments. Can modify endpoints/restricted Kf needs to modify endpoints for App deployments. Can modify deployments Kf needs to modify deployments for functionalities such as pushing Apps. Can modify mutatingwebhookconfiguration Mutatingwebhookconfiguration is needed by {{mesh_name}}, a Kf dependency, for admission webhooks. Can modify customresourcedefinitions customresourcedefinitions/status Kf manages resources through Custom Resources such as Apps, Spaces and Builds. Can modify horizontalpodautoscalers Kf supports autoscaling based on Horizontal Pod Autoscalers. Can modify namespace/finalizer Kf needs to set owner reference of webhooks. Third-party libraries Third-party library source code and licenses can be found in the /third_party directory of any Kf container image.\nYou can also run kf third-party-licenses to view the third-party licenses for the version of the Kf CLI that you downloaded.\n","categories":"","description":"","excerpt":"Kf requires Kubernetes and several other OSS projects to run. Some of …","ref":"/kf/docs/v2.11/operator/kf-dependencies/","tags":"","title":"Kf dependencies and architecture"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kf/docs/v2.11/","tags":"","title":"Documentation"},{"body":" Kf Kf offers developers the Cloud Foundry experience while empowering operators to adopt declarative Kubernetes practices. It makes migrating Cloud Foundry workloads to Kubernetes straightforward, and most importantly, avoids major changes to developer workflows. Documentation GitHub ","categories":"","description":"","excerpt":" Kf Kf offers developers the Cloud Foundry experience while empowering …","ref":"/kf/","tags":"","title":"Kf"},{"body":"Kf can benefit from the rich ecosystem of Anthos and GKE, including automation, managed backing services, and development tools.\nGKE reference architecture Kf clusters are managed just like any other GKE cluster, and access resources in the same way.\nConfigSync Kf can work with ConfigSync to automate Space creation across any number of clusters. Create new namespaces on non-Kf clusters, and Spaces on Kf clusters. You can also manage Spaces by removing the Kf CLI from Space management, if desired. Use kubectl explain to get Kf CRD specs to fully manage your product configuration via GitOps.\n","categories":"","description":"Learn about common ways of deploying and managing Kf clusters.\n","excerpt":"Learn about common ways of deploying and managing Kf clusters.\n","ref":"/kf/docs/v2.11/operator/reference-architecture/","tags":"","title":"Kf reference architecture diagrams"},{"body":"The following sections describe the Kubernetes ClusterRoles that are created by Kf and lists the permissions that are contained in each ClusterRole.\nSpace developer role The Space developer role aggregates permissions application developers use to deploy and manage applications within a Kf Space.\nYou can retrieve the permissions granted to Space developers on your cluster using the following command.\nkubectl describe clusterrole space-developer The default installation of Kf provides the following permissions:\nPolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- events [] [] [*] secrets [] [] [*] *.kf.dev [] [] [*] networkpolicies.networking.k8s.io [] [] [*] pods/exec [] [] [create] *.upload.kf.dev [] [] [create] pods/log [] [] [get list watch] pods [] [] [get list watch] rolebindings.rbac.authorization.k8s.io [] [] [get list watch] Space auditor role The Space auditor role aggregates read-only permissions that auditors and automated tools use to validate applications within a Kf Space.\nYou can retrieve the permissions granted to Space auditors on your cluster using the following command.\nkubectl describe clusterrole space-auditor The default installation of Kf provides the following permissions:\nPolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- events [] [] [*] apps.kf.dev [] [] [get list watch] rolebindings.rbac.authorization.k8s.io [] [] [get list watch] Space manager role The Space manager role aggregates permissions that allow delegation of duties to others within a Kf Space.\nYou can retrieve the permissions granted to Space managers on your cluster using the following command.\nkubectl describe clusterrole space-manager The default installation of Kf provides the following permissions:\nPolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- clusterroles.rbac.authorization.k8s.io [] [space-auditor] [bind] clusterroles.rbac.authorization.k8s.io [] [space-developer] [bind] clusterroles.rbac.authorization.k8s.io [] [space-manager] [bind] rolebindings.rbac.authorization.k8s.io [] [] [get list update patch watch] apps.kf.dev [] [] [get list watch] Note: Subjects bound to the space-manager ClusterRole within a Kf Space are also granted write access to that Space. Dynamic Space manager role Each Kf Space creates a ClusterRole with the name SPACE_NAME-manager, where SPACE_NAME-manager is called the dynamic manager role.\nKf automatically grants all subjects with the space-manager role within the Space the dynamic manager role at the cluster scope. The permissions for the dynamic manager role allow Space managers to update settings on the Space with the given name.\nYou can retrieve the permissions granted to the dynamic manager role for any Space on your cluster using the following command.\nkubectl describe clusterrole SPACE_NAME-manager The default installation of Kf provides the following permissions:\nPolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- spaces.kf.dev [] [SPACE_NAME] [get list watch update patch] Kf cluster reader role Kf automatically grants the kf-cluster-reader role to all users on a cluster that already have the space-developer, space-auditor, or space-manager role within a Space.\nYou can retrieve the permissions granted to Space Kf cluster readers on your cluster using the following command.\nkubectl describe clusterrole kf-cluster-reader The default installation of Kf provides the following permissions:\nPolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- namespaces [] [kf] [get list watch] clusterservicebrokers.kf.dev [] [] [get list watch] spaces.kf.dev [] [] [get list watch] ","categories":"","description":"Understand how Kf uses Kubernetes' RBAC to assign roles.","excerpt":"Understand how Kf uses Kubernetes' RBAC to assign roles.","ref":"/kf/docs/v2.11/operator/security/kubernetes-roles/","tags":"","title":"Kubernetes Roles"},{"body":"Kf can use GKE’s Google Cloud integrations to send a log of events to your Cloud Monitoring and Cloud Logging project for observability. For more information, see Overview of GKE operations.\nKf deploys two server side components:\nController Webhook To view the logs for these components, use the following Cloud Logging query:\nresource.type=\"k8s_container\" resource.labels.project_id=\u003cPROJECT ID\u003e resource.labels.location=\u003cGCP ZONE\u003e resource.labels.cluster_name=\u003cCLUSTER NAME\u003e resource.labels.namespace_name=\"kf\" labels.k8s-pod/app=\u003ccontroller OR webhook\u003e Note: Replace the values inside the \u003c\u003e with the correct values. ","categories":"","description":"","excerpt":"Kf can use GKE’s Google Cloud integrations to send a log of events to …","ref":"/kf/docs/v2.11/operator/logging-monitoring/logging-monitoring/","tags":"","title":"Logging and monitoring"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kf/docs/v2.11/operator/logging-monitoring/","tags":"","title":"Logging and Monitoring"},{"body":"By default, Kf includes native integration with Cloud Monitoring and Cloud Logging. When you create a cluster, both Monitoring and Cloud Logging are enabled by default. This integration lets you monitor your running clusters and help analyze your system and application performance using advanced profiling and tracing capabilities.\nApplication level performance metrics is provided by Istio sidecar injection which is injected alongside applications deployed via Kf. You can also create SLO and Alerts using this default integration to monitor performance and availability of both system and applications.\nEnsure the following are setup on your cluster:\nCloud Monitoring and Cloud Logging are enabled on the Kf cluster by default unless you disabled them explicitly, so no extra step is required.\nIstio sidecar injection is enabled. Sidecar proxy will inject application level performance metrics.\n","categories":"","description":"","excerpt":"By default, Kf includes native integration with Cloud Monitoring and …","ref":"/kf/docs/v2.11/operator/logging-monitoring/monitoring-overview/","tags":"","title":"Logging and monitoring overview"},{"body":"","categories":"","description":"Learn about common cluster networking setups.\n","excerpt":"Learn about common cluster networking setups.\n","ref":"/kf/docs/v2.11/operator/networking/","tags":"","title":"Networking"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kf/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"Learn about security considerations for your Kf cluster.","excerpt":"Learn about security considerations for your Kf cluster.","ref":"/kf/docs/v2.11/operator/security/","tags":"","title":"Security"},{"body":"All Kf Apps that serve HTTP traffic to users or applications outside of the cluster must be associated with a domain name.\nKf has three locations where domains can be configured. Ordered by precedence, they are:\nApps Spaces The config-defaults ConfigMap in the kf Namespace Edit the config-defaults ConfigMap The config-defaults ConfigMap holds cluster-wide settings for Kf and can be edited by cluster administrators. The values in the ConfigMap are read by the Spaces controller and modify their configuration. Domain values are reflected in the Space’s status.networkConfig.domains field.\nTo modify Kf cluster’s domain, edit the config-defaults ConfigMap in the kf Namespace:\nkubectl edit configmap config-defaults -n kf Add or update the entry for the spaceClusterDomain key like the following:\nspaceClusterDomain: my-domain.com To validate the configuration was updated correctly, check the domain value in a Space:\nkf space SPACE_NAME -o \"jsonpath={.status.networkConfig.domains[]['domain']}\" The output will look similar to:\nGetting Space some-space some-space.my-domain.com Each Space prefixes the cluster domains with its own name. This prevents conflicts between Apps.\nWarning: Updating the spaceClusterDomain in config-defaults will immediately be reflected on all Spaces and Apps that haven’t overridden the domain. Assign Space domains Spaces are the authoritative location for domain configuration. You can assign domains and sub-domains to each Space for developers to use. The field for configuring domains is spec.networkConfig.domains.\nUse kf space to view the domains assigned to a Space:\nkf space SPACE_NAME In the output, the Spec field contains specific configuration for the Space and the Status field reflects configuration for the Space with cluster-wide defaults appended to the end:\n... Spec: Network Config: Domains: Domain: my-space.mycompany.com ... Status: Network Config: Domains: Domain: my-space.mycompany.com Domain: my-space.prod.us-east1.kf.mycompany.com Note: Apps will use the first domain the status if developers don’t specify a domain. Add or remove domains using the CLI The kf CLI supports mutations on Space domains. Each command outputs a diff between the old and new configurations.\nAdd a new domain with kf configure-space append-domain:\nkf configure-space append-domain SPACE_NAME myspace.mycompany.com Add or make an existing domain the default with kf configure-space set-default-domain:\nkf configure-space set-default-domain SPACE_NAME myspace.mycompany.com And finally, remove a domain:\nkf configure-space remove-domain SPACE_NAME myspace.mycompany.com Note: You can override a cluster domains by inserting a new domain on the spec with the same domain. This ensures the space won’t be updated if the cluster domain changes. Use Apps to specify domains Apps can specify domains as part of their configuration. Routes are mapped to Apps during kf push using the following logic:\nlet current_routes = The set of routes already on the app let manifest_routes = The set of routes defined by the manifest let flag_routes = The set of routes supplied by the --route flag(s) let no_route = Whether the manifest has no-route:true or --no-route is set let random_route = Whether the manifest has random-route:true or --random-route is set let new_routes = Union(current_routes, manifest_routes, flag_routes) if new_routes.IsEmpty() then if random_route then new_routes.Add(CreateRandomRoute()) else new_routes.Add(CreateDefaultRoute()) end end if no_route then new_routes.RemoveAll() end return new_routes If an App doesn’t specify a Route, or requests a random Route, the first domain on the Space is used. If the first domain on a Space changes, all Apps in the Space using the default domain are updated to reflect it.\nCustomize domain templates Kf supports variable substitution in domains. Substitution allows a single cluster-wide domain to be customized per-Space and to react to changes to the ingress IP. Substitution is performed on variables with the syntax $(VARIABLE_NAME) that occur in a domain.\nVariable Description CLUSTER_INGRESS_IP The IPV4 address of the cluster ingress. SPACE_NAME The name of the Space. Note: Use the SPACE_NAME variable in domains to allow developers in different Spaces to avoid DNS conflicts if they push Apps with the same name. Examples The following examples demonstrate how domain variables can be used to support a variety of different organizational structures and cluster patterns.\nUsing a wildcard DNS service like nip.io:\n$(SPACE_NAME).$(CLUSTER_INGRESS_IP).nip.io Domain for an organization with centrally managed DNS:\n$(SPACE_NAME).cluster-name.example.com Domain for teams who manage their own DNS:\ncluster-name.$(SPACE_NAME).example.com Domain for a cluster with warm failover and external circuit breaker:\n$(SPACE_NAME)-failover.cluster-name.example.com Differences between Kf and CF Kf Spaces prefix the cluster-wide domain with the Space name. Kf does not check for domain conflicts on user-specified routes. ","categories":"","description":"Learn to set up a DNS domain apps can use on your cluster.","excerpt":"Learn to set up a DNS domain apps can use on your cluster.","ref":"/kf/docs/v2.11/operator/networking/setting-up-a-custom-domain/","tags":"","title":"Set up a custom domain"},{"body":"You can secure the ingress gateway with HTTPS by using simple TLS, and enable HTTPS connections to specific webpages. In addition, you can redirect HTTP connections to HTTPS.\nHTTPS creates a secure channel over an insecure network, protecting against man-in-the-middle attacks and encrypting traffic between the client and server. To prepare a web server to accept HTTPS connections, an administrator must create a public key certificate for the server. This certificate must be signed by a trusted certificate authority for a web browser to accept it without warning.\nNote: These instructions supplement the Istio instructions to configure a TLS ingress gateway, and assume that a valid certificate and private key for the server have already been created. Edit the gateway named external-gateway in the kf namespace using the built-in Kubernetes editor:\nkubectl edit gateway -n kf external-gateway Assuming you have a certificate and key for your service, create a Kubernetes secret for the ingress gateway. Make sure the secret name does not begin with istio or prometheus. For this example, the secret is named myapp-https-credential. Under servers: Add a section for port 443. Under tls:, set the credentialName to the name of the secret you just created. Under hosts:, add the host name of the service you want to secure with HTTPS. This can be set to an entire domain using a wildcard (e.g. *.example.com) or scoped to just one hostname (e.g. myapp.example.com). There should already be a section under servers: for port 80 HTTP. Keep this section in the Gateway definition if you would like all traffic to come in as HTTP. To redirect HTTP to HTTPS, add the value httpsRedirect: true under tls in the HTTP server section. See the Istio Gateway documentation for reference. Note that adding this in the section where hosts is set to * means that all traffic is redirected to HTTPS. If you only want to redirect HTTP to HTTPS for a single app/domain, add a separate HTTP section specifying the redirect. Shown below is an example of a Gateway spec that sets up HTTPS for myapp.example.com and redirects HTTP to HTTPS for that host:\nspec: selector: istio: ingressgateway servers: - hosts: - myapp.example.com port: name: https number: 443 protocol: HTTPS tls: credentialName: myapp-https-credential mode: SIMPLE - hosts: - myapp.example.com port: name: http-my-app number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - '*' port: name: http number: 80 protocol: HTTP ","categories":"","description":"Learn to set up a TLS certificate to enable HTTPS.","excerpt":"Learn to set up a TLS certificate to enable HTTPS.","ref":"/kf/docs/v2.11/operator/networking/setting-up-https-ingress/","tags":"","title":"Set up HTTPS ingress"},{"body":"Kf integrates tightly with Kubernetes and Istio to provide robust network policy enforcement.\nBy default, Kf workloads are run in the Kubernetes cluster and resolve addresses using Kubernetes DNS. This DNS resolver will first attempt to resolve addresses within the cluster, and only if none are found will attempt external resolution.\nEach Kf App gets run with an Envoy sidecar injected by Istio or the Anthos Service Mesh (ASM). This sidecar proxies all network traffic in and out of the Kubernetes Pod.\nEach Kubernetes Pod is executed on a Node, a physical or virtual machine responsible for managing the container images that make up a Pod. Nodes exist on a physical or virtual network.\nTogether, these form a hierarchy of systems you can apply network policies. These are listed below from least to most granular.\nNetwork level policies Workload protection starts with the network your GKE cluster is installed on.\nIf you’re running Kf on a GKE cluster on GCP, Kf recommends:\nPlacing your GKE cluster on a Virtual Private Cloud (VPC) network. With Private Google Access enabled. Using Cloud NAT to control egress. Node level policies You can set up policies for containers running on the Node using Kubernetes NetworkPolicies. These are the closest mapping to Cloud Foundry network policies that exist in Kubernetes.\nNetworkPolicies are backed by a Kubernetes add-on. If you set up your own GKE cluster, you will need to enable NetworkPolicy enforcement.\nNote: NetworkPolicies applied by Project Calico filter on a per-connection basis. On Linux they are backed by iptables. Like any firewall, they are not a substitute of authentication and authorization. Kf labels Apps with kf.dev/networkpolicy=app and builds with kf.dev/networkpolicy=build. This allows you to target NetworkPolicies directly at Pods running Apps or Builds.\nEach Kf Space creates two NetworkPolicies to start with, one targeting Apps and one targeting Builds. You can change the configuration on the Space’s spec.networkConfig.(app|build)NetworkPolicy.(in|e)gress fields. These fields can be set to one of the following values:\nEnum Value Description PermitAll Allows all traffic. DenyAll Denies all traffic. By default Kf uses a permissive network policy. This allows the following functionality that Kf uses:\nNorth/South routing to the cluster ingress gateway Egress to the Internet e.g. to fetch Buildpacks East/West routing between Apps Access to the Kubernetes DNS server Access to container registries Direct access to the VPC network Access to Google services like Cloud Logging Access to the Workload Identity server for automatic rotating credentials Note: Setting a default DenyAll policy will break existing and new Apps and Builds unless you create additional NetworkPolicies to add back the connections listed above. Service mesh policies If you need fine-grained networking control, authentication, authorization, and observability you can apply policies using Anthos Service Mesh.\nA service mesh is an infrastructure layer that enables managed, observable and secure communication across your services, letting you create robust enterprise applications made up of many microservices on your chosen infrastructure.\nYou can see the list of supported features here.\n","categories":"","description":"Learn to set up a Kubernetes NetworkPolicies to configure traffic.","excerpt":"Learn to set up a Kubernetes NetworkPolicies to configure traffic.","ref":"/kf/docs/v2.11/operator/networking/setting-up-networkpolicies/","tags":"","title":"Set up network policies"},{"body":"Kf supports Kubernetes native NFS, and exposes them with a dedicated nfsvolumebroker service broker for developers to consume. This broker has an nfs service offering which has a service plan named existing.\nUse kf marketplace to see the service offering:\n$ kf marketplace ... Broker Name Namespace Description nfsvolumebroker nfs mout nfs shares ... Use kf marketplace -s nfs to see the service plan:\n$ kf marketplace -s nfs ... Broker Name Free Description nfsvolumebroker existing true mount existing nfs server ... Requirements You need an NFS volume that can be accessed by your Kubernetes cluster. For example Cloud Filestore which Google’s managed NFS solution that provides access to clusters in the same gcloud project.\nPrepare NFS If you have an existing NFS service, you can use that. If you want a Google managed NFS service, create a Filestore instance and Kf will automaticlaly configure the cluster to use it.\nWarning: You only need to create the NFS instance. Kf will create relevant Kubernetes objects, including PersistentVolume and PersistentVolumeClaims. Do not manually mount the volume.\nWhat’s next? Configure NFS volumes ","categories":"","description":"","excerpt":"Kf supports Kubernetes native NFS, and exposes them with a dedicated …","ref":"/kf/docs/v2.11/operator/service-brokers/nfs-platform-setup/","tags":"","title":"Set up NFS platform"},{"body":"Learn about configuring stacks and buildpacks for your platform.\n","categories":"","description":"","excerpt":"Learn about configuring stacks and buildpacks for your platform.\n","ref":"/kf/docs/v2.11/operator/buildpacks/","tags":"","title":"Stacks and Buildpacks"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kf/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"Learn how to run one-off jobs using Kf.\n","excerpt":"Learn how to run one-off jobs using Kf.\n","ref":"/kf/docs/v2.11/developer/tasks/","tags":"","title":"Tasks"},{"body":"Kf provides you with several types of logs. This document describes these logs and how to access them.\nApplication logs All logs written to standard output stdout and standard error stderr, are uploaded to Cloud Logging and stored under the log name user-container.\nOpen Cloud Logging and run the following query:\nresource.type=\"k8s_container\" log_name=\"projects/YOUR_PROJECT_ID/logs/user-container\" resource.labels.project_id=YOUR_PROJECT_ID resource.labels.location=GCP_COMPUTE_ZONE (e.g. us-central1-a) resource.labels.cluster_name=YOUR_CLUSTER_NAME resource.labels.namespace_name=YOUR_KF_SPACE_NAME resource.labels.pod_name:YOUR_KF_APP_NAME You should see all your application logs written on standard stdout and standard error stderr.\nAccess logs for your applications Kf provides access logs using Istio sidecar injection. Access logs are stored under the log name server-accesslog-stackdriver.\nOpen Cloud Logging and run the following query:\nresource.type=\"k8s_container\" log_name=\"projects/YOUR_PROJECT_ID/logs/server-accesslog-stackdriver\" resource.labels.project_id=YOUR_PROJECT_ID resource.labels.location=GCP_COMPUTE_ZONE (e.g. us-central1-a) resource.labels.cluster_name=YOUR_CLUSTER_NAME resource.labels.namespace_name=YOUR_KF_SPACE_NAME resource.labels.pod_name:YOUR_KF_APP_NAME You should see access logs for your application. Sample access log:\n{ \"insertId\": \"166tsrsg273q5mf\", \"httpRequest\": { \"requestMethod\": \"GET\", \"requestUrl\": \"http://test-app-38n6dgwh9kx7h-c72edc13nkcm.***. ***.nip.io/\", \"requestSize\": \"738\", \"status\": 200, \"responseSize\": \"3353\", \"remoteIp\": \"10.128.0.54:0\", \"serverIp\": \"10.48.0.18:8080\", \"latency\": \"0.000723777s\", \"protocol\": \"http\" }, \"resource\": { \"type\": \"k8s_container\", \"labels\": { \"container_name\": \"user-container\", \"project_id\": ***, \"namespace_name\": ***, \"pod_name\": \"test-app-85888b9796-bqg7b\", \"location\": \"us-central1-a\", \"cluster_name\": *** } }, \"timestamp\": \"2020-11-19T20:09:21.721815Z\", \"severity\": \"INFO\", \"labels\": { \"source_canonical_service\": \"istio-ingressgateway\", \"source_principal\": \"spiffe://***.svc.id.goog/ns/istio-system/sa/istio-ingressgateway-service-account\", \"request_id\": \"0e3bac08-ab68-408f-9b14-0aec671845bf\", \"source_app\": \"istio-ingressgateway\", \"response_flag\": \"-\", \"route_name\": \"default\", \"upstream_cluster\": \"inbound|80|http-user-port|test-app.***.svc.cluster.local\", \"destination_name\": \"test-app-85888b9796-bqg7b\", \"destination_canonical_revision\": \"latest\", \"destination_principal\": \"spiffe://***.svc.id.goog/ns/***/sa/sa-test-app\", \"connection_id\": \"82261\", \"destination_workload\": \"test-app\", \"destination_namespace\": ***, \"destination_canonical_service\": \"test-app\", \"upstream_host\": \"127.0.0.1:8080\", \"log_sampled\": \"false\", \"mesh_uid\": \"proj-228179605852\", \"source_namespace\": \"istio-system\", \"requested_server_name\": \"outbound_.80_._.test-app.***.svc.cluster.local\", \"source_canonical_revision\": \"asm-173-6\", \"x-envoy-original-dst-host\": \"\", \"destination_service_host\": \"test-app.***.svc.cluster.local\", \"source_name\": \"istio-ingressgateway-5469f77856-4n2pw\", \"source_workload\": \"istio-ingressgateway\", \"x-envoy-original-path\": \"\", \"service_authentication_policy\": \"MUTUAL_TLS\", \"protocol\": \"http\" }, \"logName\": \"projects/*/logs/server-accesslog-stackdriver\", \"receiveTimestamp\": \"2020-11-19T20:09:24.627065813Z\" } Audit logs Audit Logs provides a chronological record of calls that have been made to the Kubernetes API Server. Kubernetes audit log entries are useful for investigating suspicious API requests, for collecting statistics, or for creating monitoring alerts for unwanted API calls.\nOpen Cloud Logging and run the following query:\nresource.type=\"k8s_container\" log_name=\"projects/YOUR_PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivity\" resource.labels.project_id=YOUR_PROJECT_ID resource.labels.location=GCP_COMPUTE_ZONE (e.g. us-central1-a) resource.labels.cluster_name=YOUR_CLUSTER_NAME protoPayload.request.metadata.name=YOUR_APP_NAME protoPayload.methodName:\"deployments.\" You should see a trace of calls being made to the Kubernetes API server.\nConfigure logging access control Follow these instructions to provide logs access to developers and other members on the team. The role roles/logging.viewer provides read-only access to logs.\nUse Logs Router You can also use Logs Router to route the logs to supported destinations.\n","categories":"","description":"","excerpt":"Kf provides you with several types of logs. This document describes …","ref":"/kf/docs/v2.11/operator/logging-monitoring/monitoring-logging/","tags":"","title":"View logs"}]